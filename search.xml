<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>test again</title>
      <link href="/2023/03/18/test-again/"/>
      <url>/2023/03/18/test-again/</url>
      
        <content type="html"><![CDATA[<p>继续测试图片</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora//20230318215431_uXq0kP_image-20230318215431316.png" alt="image-20230318215431316"></p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora//20230318220802_i5SLeB_image-20230318220801946.png" alt="图片描述啊soga"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>test</title>
      <link href="/2023/03/17/test/"/>
      <url>/2023/03/17/test/</url>
      
        <content type="html"><![CDATA[<p>测试图片能不能显示</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora//20230318211453_6bHbxo_image-20230318211452774.png" alt="image-20230318211452774"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>并行程序设计-并行复习</title>
      <link href="/2023/02/10/bing-xing-fu-xi/"/>
      <url>/2023/02/10/bing-xing-fu-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="并行复习"><a href="#并行复习" class="headerlink" title="并行复习"></a>并行复习</h1><h2 id="并行硬件与并行软件"><a href="#并行硬件与并行软件" class="headerlink" title="并行硬件与并行软件"></a>并行硬件与并行软件</h2><h3 id="冯诺依曼瓶颈"><a href="#冯诺依曼瓶颈" class="headerlink" title="冯诺依曼瓶颈"></a>冯诺依曼瓶颈</h3><p>cpu去主存储器中去指令的过程比cpu执行指令要慢很多</p><p>三方面改进：</p><h4 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h4><p>CPU Cache是一组相比于 CPU 主存更能快速访问的内存区域</p><h4 id="虚拟内存（了解）"><a href="#虚拟内存（了解）" class="headerlink" title="虚拟内存（了解）"></a>虚拟内存（了解）</h4><p>主存中放不下，把不常用的放到虚拟内存中</p><h4 id="低层次并行"><a href="#低层次并行" class="headerlink" title="低层次并行"></a>低层次并行</h4><p>1）指令级并行 </p><p>不是多核</p><p>让多个处理器或者功能单元 同时执行指令</p><p>两种实现方式</p><ul><li><p>流水线</p><ul><li><p>![image-20230220164322035](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220164322035.png)</p><p>7000ns➡️1006ns</p></li></ul></li><li><p>多发射</p><ul><li>复制功能单元来同时执行程序中的不同指令</li><li>静态多发射：功能单元在<strong>编译</strong>时调度</li><li>动态多发射：功能单元在运行时间调度。  <strong>超标量</strong></li></ul></li></ul><p>超标量</p><p>必须找出能同时执行的指令</p><p>猜测</p><p>![image-20230220170043087](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220170043087.png)</p><p>2）硬件多线程</p><p>分为3种</p><ul><li>细粒度 每条指令完成后都切换</li><li>粗粒度 只切换那些需要等待较长时间而阻塞的线程</li><li>同步多线程/超线程 真正意义上的同时 一个核上多个线程</li></ul><h3 id="Cache相关概念"><a href="#Cache相关概念" class="headerlink" title="Cache相关概念"></a>Cache相关概念</h3><p>局部性原理</p><p>程序访问完一个存储区域往往会访问接下来的区域</p><ul><li>空间的局部性</li><li>时间的局部性</li></ul><p>cache命中</p><p>cache缺失 </p><p>cache的问题</p><ul><li>写直达：当CPU向Cache中写数据时，高速缓存行会立即写入主存中</li><li>写回：数据不是立即更新到主存中，而是将发生数据更新的高速缓存行标记为脏。当发生高速缓存行替换时，标记为脏的高速缓存行被写入主存中</li></ul><p>Cache一致性</p><p>![image-20230220203121263](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220203121263.png)</p><p>z1的值取决于什么时候更新x</p><p>解决</p><ul><li>监听Cache一致性协议</li><li>基于目录的Cache一致性协议</li></ul><p>![image-20230221163041208](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230221163041208.png)</p><h3 id="并行、多线程相关概念"><a href="#并行、多线程相关概念" class="headerlink" title="并行、多线程相关概念"></a>并行、多线程相关概念</h3><p>1.1.3</p><h3 id="SIMD-MIMD（共享内存、分布式内存、网络连接等）"><a href="#SIMD-MIMD（共享内存、分布式内存、网络连接等）" class="headerlink" title="SIMD MIMD（共享内存、分布式内存、网络连接等）"></a>SIMD MIMD（共享内存、分布式内存、网络连接等）</h3><p>Flynn‘s分类法</p><p> ![image-20230220171506418](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220171506418.png)</p><h4 id="SIMD"><a href="#SIMD" class="headerlink" title="SIMD"></a>SIMD</h4><p>数据并行</p><p>将数据分配给多个处理器实现并行</p><p>相同指令来操作数据子集</p><p>缺点</p><ul><li>所有ALU要么执行相同的指令，要么同时处于空闲状态</li><li>ALU同步操作</li><li>ALU没有指令存储器 一次只能执行一条指令</li><li>SIMD并行在大型数据并行问题上有用，但是处理其他并行问题并不优秀</li></ul><p>向量处理器</p><p>对数组或者数据向量进行操作</p><p>向量寄存器 能够存储多个数组成的向量并操作</p><p>向量指令 在向量上操作</p><p>向量化和流水化的功能单元 对向量中每个元素做同样的操作</p><p>优点</p><ul><li>速度快 容易使用 向量编译器擅长识别向量化的代码</li><li>编译器能够提供代码为什么不能向量化的原因</li><li>内存带宽高 每个加载的数据都会使用</li></ul><p>缺点</p><ul><li>不能处理不规则数据结构和其他的并行结构</li><li>可扩展性受限   可扩展性：处理更大问题的能力</li></ul><p>GPU</p><p><strong>使用SIMD并行</strong></p><p>并不是纯粹的SIMD系统</p><h4 id="MIMD"><a href="#MIMD" class="headerlink" title="MIMD"></a>MIMD</h4><p>多指令流多数据流</p><p>完全独立的处理单元或者核</p><p>每个处理单元或者核都有自己的控制单元和ALU</p><p>分为：</p><p>1）共享内存系统</p><p>内存共享</p><p>处理器通过访问共享的内存来隐式通信</p><p>一个/多个多核处理器 一块芯片上有多个CPU或者核</p><p>![image-20230220175039016](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220175039016.png)</p><p>分为两类</p><ul><li><p>一致内存访问系统</p><p>![image-20230220190140406](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220190140406.png)</p></li><li><p>非一致内存访问系统</p><p>![image-20230220190203183](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220190203183.png)</p></li></ul><p>2）分布式内存系统</p><p>集群</p><p>大多数混合系统</p><p>![image-20230220190316207](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220190316207.png)</p><h4 id="互连网络"><a href="#互连网络" class="headerlink" title="互连网络"></a>互连网络</h4><p>分为两类</p><ul><li>共享内存互连网络</li><li>分布式内存互连网络</li></ul><h5 id="共享内存互连网络"><a href="#共享内存互连网络" class="headerlink" title="共享内存互连网络"></a>共享内存互连网络</h5><p>1）总线互连</p><p>随着连接到总线的设备数量增加，竞争增加，性能会下降</p><p>2）交换互连网络</p><p>交换器</p><p>交叉开关矩阵  </p><p>![image-20230220191320436](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220191320436.png)</p><h5 id="分布式内存互连网络"><a href="#分布式内存互连网络" class="headerlink" title="分布式内存互连网络"></a>分布式内存互连网络</h5><p>分为两种</p><ul><li>直接互连 </li><li>间接互连</li></ul><p>1）直接互连</p><p>每个交换器都与一个处理器-内存对直接相连，交换器之间也互相连接</p><p>![image-20230220191821362](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220191821362.png)</p><p>和总线互连不同：P1，P2，P3可以同时信息交换，因为有交换器控制</p><p>2）间接互连（了解）</p><p>交换器不一定与处理器直接相连</p><p>例子</p><ul><li>交叉开关矩阵  和共享内存网络中的交叉矩阵不同   </li><li>Omega网络</li></ul><h5 id="评价网络好坏-（掌握）"><a href="#评价网络好坏-（掌握）" class="headerlink" title="评价网络好坏 （掌握）"></a>评价网络好坏 （掌握）</h5><p>等分<strong>宽度</strong>  针对分布式内存网络 中的 直接互连网络</p><p>衡量同时通信的链路数目 连接性</p><p>计算方法：计数去除最少的链路数从而将节点分为两份</p><p>环 等分宽度=2</p><p>![image-20230220194912747](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220194912747.png)</p><p>二维网格结构 p为处理器数目 </p><p>![image-20230220194747171](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220194747171.png)</p><p>带宽：传输数据的速度</p><p>等分带宽=带宽✖️等分宽度</p><p>如果在环中，链路的带宽是10亿位每秒，等分带 宽就是20亿位每秒</p><p>全相连网络 </p><p>![image-20230220195555960](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220195555960.png)</p><p>超立方体</p><p>递归构造</p><p>维度 d。</p><p>节点数<br>$$<br>p=2^d<br>$$<br>等分宽度<br>$$<br>p/2<br>$$</p><p>总结</p><table><thead><tr><th align="center">结构</th><th>环</th><th>二维环面</th><th></th><th>超立方体</th><th>全相连</th></tr></thead><tbody><tr><td align="center">等分宽度</td><td>2</td><td>![image-20230220200426667](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220200426667.png)</td><td></td><td>p/2</td><td>![image-20230220200530472](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220200530472.png)</td></tr></tbody></table><h3 id="并行程序设计的复杂性"><a href="#并行程序设计的复杂性" class="headerlink" title="并行程序设计的复杂性"></a>并行程序设计的复杂性</h3><ul><li>足够的并发度</li><li>并发粒度<ul><li>独立的计算任务的大小</li></ul></li><li>局部性<ul><li>临近数据</li></ul></li><li>负载均衡<ul><li>处理器任务量相近</li></ul></li><li>协调和同步</li></ul><h3 id="并行算法设计（竞争条件-数据依赖-同步等概念）"><a href="#并行算法设计（竞争条件-数据依赖-同步等概念）" class="headerlink" title="并行算法设计（竞争条件 数据依赖 同步等概念）"></a>并行算法设计（竞争条件 数据依赖 同步等概念）</h3><p>额外开销</p><ul><li>进程间通信 最大开销</li><li>进程空闲 负载不均、同步操作、不能并行化的部分</li><li>额外计算</li></ul><p>W8L1里面</p><p>看书</p><p>或者W4L1乱七八糟</p><p>飞书共享空间ppt上没有</p><p>原子性:一组操作要么全部执行，要么全不执行。</p><p>临界区:一个更新共享资源的<strong>代码段</strong>，一次只能允许一个线程执行该代码段</p><p>竞争条件:多个进程/线程尝试<strong>更新同一个共享资源</strong>时，结果可能是无法预测的，则存在竞争条件。</p><p>数据依赖(data dependence):两个<strong>内存操作的序</strong>，为了保证结果的正确性，必须保持这个序。</p><p>同步(synchronization) :在时间上强制使执各行进程/线程在某一点等待，确保各进程/线程的正常顺 序和对共享可写数据的正确访问。</p><p>互斥:任何时刻都只有一个线程在执行<br>互斥量:互斥锁的简称，用来限制每次只有一个线程能进入临界区。保证了一个线程独享临界区。<br>障碍:阻碍线程继续执行，在此程序点等待，直到所有参与线程都达到障碍点才能继续执行</p><h3 id="并行算法性能分析（加速比-效率-可扩展性-阿姆达尔定律）"><a href="#并行算法性能分析（加速比-效率-可扩展性-阿姆达尔定律）" class="headerlink" title="并行算法性能分析（加速比 效率 可扩展性 阿姆达尔定律）"></a>并行算法性能分析（加速比 效率 可扩展性 阿姆达尔定律）</h3><p>串行：时间复杂度</p><h4 id="加速比。"><a href="#加速比。" class="headerlink" title="加速比。"></a>加速比。</h4><p>谁比谁‼️ 串行比并行</p><p>![image-20230220215838708](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220215838708.png)</p><p>选最优（没有最优的话选择普遍使用的）</p><p>例子：n个数相加</p><p>串行 O（n）</p><p>并行 树形结构 深度为logn 包括加法时间   数据传输时间=启动传输时间+传输过程时间 Tp=O（3logn）</p><p>![image-20230220220921233](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220220921233.png)</p><p>例子</p><p>串行起泡排序150s 串行快速排序30s 并行起泡排序40</p><p>S=<strong>30</strong>/40=0.75</p><p>p为核数</p><ul><li>一般S&lt;=p</li><li>S=p 称该并行算法有<strong>线性加速比</strong></li><li>S&gt;p 超线性加速比 （了解即可</li></ul><h4 id="阿姆达尔定律"><a href="#阿姆达尔定律" class="headerlink" title="阿姆达尔定律"></a>阿姆达尔定律</h4><p>可并行化比例有限</p><p>![image-20230220223405214](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220223405214.png)</p><p>只和a和p有关系，和时间没关系</p><p>例子</p><p>![image-20230220223151683](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220223151683.png)</p><p>S&lt;10</p><p>阿姆达尔定律：通过并行化产生的加速比受限</p><h4 id="效率"><a href="#效率" class="headerlink" title="效率"></a>效率</h4><p>![image-20230220223729564](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230220223729564.png)</p><p> 例子</p><p>串行排序算法例子🌟</p><h4 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h4><p>强可扩展。  问题规模不变。 很难达到</p><p>弱可扩展   问题规模增大</p><h2 id="SIMD-1"><a href="#SIMD-1" class="headerlink" title="SIMD"></a>SIMD</h2><p>适合应用的特点</p><ul><li>连续存储的</li><li>短数据类型 8、16、32位</li><li>流式数据处理 时间局部性 数据流重用</li></ul><h3 id="选择"><a href="#选择" class="headerlink" title="选择"></a>选择</h3><p>应用 </p><p>SSE AVX CUDA GPU 协处理器Xeon Phi 没有占压倒优势 科学计算</p><p>向量长度=寄存器宽度/类型大小</p><p>C/C++：内置函数、 intrinsics</p><h3 id="SIMD编程的问题（打包解包、对其开销、控制流开销）"><a href="#SIMD编程的问题（打包解包、对其开销、控制流开销）" class="headerlink" title="SIMD编程的问题（打包解包、对其开销、控制流开销）"></a>SIMD编程的问题（打包解包、对其开销、控制流开销）</h3><p>打包解包开销</p><p>打包 拷贝到连续内存区域</p><p>解包 拷贝回内存</p><p>对齐开销</p><p>未对齐的数据可能产生不正确的结果a</p><p>控制流 所有语句必须执行</p><p>分支语句</p><p>存在控制流问题时，SIMD不是一个好的编程模型</p><h3 id="SIMD编程-W6L1"><a href="#SIMD编程-W6L1" class="headerlink" title="SIMD编程  W6L1"></a>SIMD编程  W6L1</h3><p>X86 32位</p><p>X64 64位 </p><p>SSE <strong>8个128位</strong>的向量寄存器</p><p>AVX <strong>16个256位</strong>的向量寄存器</p><p>MMX 8个64位</p><p>支持数据类型 128位=16字节</p><p>整数 16字节、8short</p><table><thead><tr><th>数据类型</th><th>名字</th><th>字节</th><th>能放几个</th></tr></thead><tbody><tr><td>dqword</td><td></td><td>16</td><td>1</td></tr><tr><td>short</td><td></td><td>2</td><td>8</td></tr><tr><td>int</td><td></td><td>4</td><td>4</td></tr><tr><td>long</td><td></td><td>8</td><td>2</td></tr><tr><td>float</td><td>单精度浮点数</td><td>4</td><td>4</td></tr><tr><td>double</td><td>双精度浮点数</td><td>8</td><td>2</td></tr><tr><td>char</td><td></td><td>2</td><td></td></tr></tbody></table><p>SSE指令</p><ul><li>数据移动指令 将数据移入/出向量寄存器</li><li>算术指令 多个数据（2doubles，4floats上的算术指令）</li><li>逻辑指令 多个数据上的逻辑运算</li><li>比较指令 多个数据上的比较运算</li><li>洗牌指令 在SIMD寄存器内移动数据</li></ul><p>SSE版本向量乘法</p><p>![image-20230221152105512](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230221152105512.png)</p><p>![image-20230221152048150](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230221152048150.png)</p><h2 id="Pthread"><a href="#Pthread" class="headerlink" title="Pthread"></a>Pthread</h2><p>并行程序设计的复杂性</p><p>Pthread一些基础API</p><p>同步相关概念</p><p>![image-20230221200946496](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230221200946496.png)</p><p>忙等待 互斥量 信号量 障碍 </p><p>负载均衡 任务划分</p><p>![image-20230224185347606](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230224185347606.png)</p><h2 id="MPI"><a href="#MPI" class="headerlink" title="MPI"></a>MPI</h2><p>![image-20230224194958501](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230224194958501.png)</p><p>隔离了独立地址空间</p><p>不会有数据竞争 但是可能有通信错误</p><p>暴露了执行模型，迫使程序员思考局部性，两点对性能都有好处</p><p>代码复杂</p><p>无共享变量 通信、同步都需要调用函数完成</p><p>提供以下类别的函数</p><p>通信</p><p>点对点通信 消息从特定的发送进程到特定的接受进程</p><p>多处理器参与的组通信</p><p>同步</p><p>障碍</p><p>无锁 没有共享变量需要保护</p><p>查询</p><p>允许接受的量比指定少，接收到更多数据就是错误</p><p>强数据类型传输</p><p>消息中的数据用三元组（地址、个数、类型）描述</p><p>send receive完成了数据传输 同步</p><h3 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h3><p>![image-20230224215719598](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230224215719598.png)</p><h3 id="混合编程"><a href="#混合编程" class="headerlink" title="混合编程"></a>混合编程</h3><p>![image-20230224221353584](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230224221353584.png)</p><p>所有MPI实现均支持MPI_THREAD_SINGLE</p><h2 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h2><h3 id="推动并行计算的原因"><a href="#推动并行计算的原因" class="headerlink" title="推动并行计算的原因"></a>推动并行计算的原因</h3><p>频率已经不是处理器发展的主角 </p><p>功耗/散热的限制 </p><p>性能上升放缓 </p><p>多核、众核成为之后CPU的发展趋势</p><h3 id="超算发展"><a href="#超算发展" class="headerlink" title="超算发展"></a>超算发展</h3><p>Top 1 Frontier：性能最强，能效最高</p><p>我国Top1：神威·太湖之光 自研的 2016</p><p>2020年，Summit，获得戈登贝尔奖的是中美合 作的团队 夺冠，“基于机器学习将分子动力学 计算精度极限推广到1亿个原子”</p>]]></content>
      
      
      <categories>
          
          <category> 并行程序设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并行程序设计 </tag>
            
            <tag> 知识总结 </tag>
            
            <tag> S5复习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并行程序设计-实验5.MPI编程练习</title>
      <link href="/2022/11/28/mpi-bian-cheng-lian-xi-shi-yan-bao-gao/"/>
      <url>/2022/11/28/mpi-bian-cheng-lian-xi-shi-yan-bao-gao/</url>
      
        <content type="html"><![CDATA[<h1 id="MPI-编程练习实验报告"><a href="#MPI-编程练习实验报告" class="headerlink" title="MPI 编程练习实验报告"></a>MPI 编程练习实验报告</h1><h2 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h2><ul><li>实现第5章课件中的梯形积分法的MPI编程熟悉并掌握MPI编程方法，规模自行设定，可探讨不同规模对不同实现方式的影响。</li><li>对于课件中“多个数组排序”的任务不均衡案例进行MPI编程实现，规模可自己设定、调整。</li><li>附加：实现高斯消去法解线性方程组的MPI编程，与SSE（或AVX）编程结合，并与Pthread、OpenMP（结合SSE或AVX）版本对比，规模自己设定。</li></ul><h2 id="实验一：梯形积分"><a href="#实验一：梯形积分" class="headerlink" title="实验一：梯形积分"></a>实验一：梯形积分</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>本实验分别使用MPI、Pthread和OpenMP三种方法，实现了梯形积分法。并通过调整梯形积分法划分成小梯形的个数规模，来比较不同编程方式的异同。</p><p>具体案例为：对于下图中给出的梯形积分法，实现并行编程。每个线程或进程计算a到b区间中的某一段的梯形面积，最后求取全局和得到结果。</p><p>![image-20230102165554905](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230102165554905.png)</p><h3 id="程序设计与实现"><a href="#程序设计与实现" class="headerlink" title="程序设计与实现"></a>程序设计与实现</h3><h4 id="MPI实现"><a href="#MPI实现" class="headerlink" title="MPI实现"></a>MPI实现</h4><p>实验共用到了3台主机的进程进行计算。每个进程根据自己的my_rank进程号得到自身的计算任务，完成局部和的计算后，（除0号进程外）使用MPI_Send将结果发送至0号进程；0号进程使用MPI_Recv阻塞式地接受其他进程传回的结果，并计算全局和。同时，0号进程还完成计时、输出结果的任务。具体的实现代码如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;stdio.h&gt;#include &lt;algorithm&gt;#include&lt;cstdlib&gt;#include&lt;ctime&gt;#include&lt;mpi.h&gt;//待积分的f(x)double f(double x) {    return 1;}//计算局部和double Trap(double a, double b, int count, double h) {    double my_result, x;    int i;    my_result = (f(a) + f(b)) / 2.0;    //局部求和，避免过多通信    for (i = 1; i &lt; count; i++) {        x = a + i * h;        my_result += f(x);    }    my_result *= h;    return my_result;}//主函数int main(){    int my_rank, comm_sz, n = 1000;    double a = 0, b = 5000;    double h = (b - a) / n;    double global_result;    clock_t start, end;    double time;    MPI_Init(NULL, NULL);    MPI_Comm_rank(MPI_COMM_WORLD, &amp;my_rank);    MPI_Comm_size(MPI_COMM_WORLD, &amp;comm_sz);    //0号进程开始计时    if (my_rank == 0) {        start = clock();    }    int local_n = n / comm_sz;    int local_a = a + my_rank * local_n * h;    int local_b = local_a + local_n * h;    //将未除尽的任务数分配给最后一个进程    if (my_rank == comm_sz - 1) {        local_n = n - (comm_sz - 1) * local_n;        local_b = b;    }    //每个进程计算自身分配的任务的局部和    double local_result = Trap(local_a, local_b, local_n, h);    //其他进程将局部和发送给0号进程    if (my_rank != 0) {        MPI_Send(&amp;local_result, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);    }    //0号进程收集局部和并求全局和    else {        global_result = local_result;        for (int source = 1; source &lt; comm_sz; source++)        {            MPI_Recv(&amp;local_result, 1, MPI_DOUBLE, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);            global_result += local_result;        }        //完成计算，0号进程停止计时        end = clock();        time = (end - start) / CLOCKS_PER_SEC;    }    //0号进程打印出结果    if (my_rank == 0) {        printf("划分成小梯形的块数: %d\n", n);        printf("计算结果是: %.15e\n", global_result);        printf("总共耗时: %f\n\nms", time);    }    MPI_Finalize();    return 0;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Pthread实现"><a href="#Pthread实现" class="headerlink" title="Pthread实现"></a>Pthread实现</h4><p>对于每个线程，先求局部和最后全局求和，避免过多通信。同时注意给全局和加锁保证结果的正确性。代码实现如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void pthread_trap(double a,double b,int n,double*global_result_p,int thread_count){    pthread_param params[thread_count];    pthread_t threads[thread_count];    pthread_mutex_t amutex;    pthread_mutex_init(&amp;amutex,NULL);    for(int i=0;i&lt;thread_count;i++){        //为每个线程参数赋值        params[i].a=a;        params[i].b=b;        params[i].n=n;        params[i].global_result_p=global_result_p;        params[i].thread_count=thread_count;        params[i].my_rank=i;        params[i].amutex=&amp;amutex;        //创建线程        pthread_create(&amp;threads[i],NULL,pthread_trap,(void*)&amp;params[i]);    }    //销毁线程    for(int i=0;i&lt;thread_count;i++){        pthread_join(threads[i],NULL);    }    //销毁互斥量锁    pthread_mutex_destroy(&amp;amutex);}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中每个线程执行的函数如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void* pthread_trap(void* p){    //恢复获取参数    pthread_param* params=(pthread_param*) p;    double h,x,my_result;    double local_a,local_b;    int local_n;    int my_rank=params-&gt;my_rank;    int thread_count=params-&gt;thread_count;    h=(params-&gt;b-params-&gt;a)/params-&gt;n;    local_n=params-&gt;n/thread_count;    local_a=params-&gt;a+my_rank*local_n*h;    local_b=local_a+local_n*h;    my_result=(f(local_a)+f(local_b))/2.0;    //先局部求和，避免过多通信    for(int i=1;i&lt;=local_n-1;i++){        x=local_a+i*h;        my_result+=f(x);    }    my_result=my_result*h;    //使用互斥量 amutex 上锁保证全局求和正确性    pthread_mutex_lock(params-&gt;amutex);    *params-&gt;global_result_p+=my_result;    pthread_mutex_unlock(params-&gt;amutex);}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="OpenMP实现"><a href="#OpenMP实现" class="headerlink" title="OpenMP实现"></a>OpenMP实现</h4><p>利用OpenMP的相关方法实现多线程并行编程，对于每个线程，先求局部和最后全局求和，避免过多通信。同时注意给全局和加锁保证结果的正确性。代码实现如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#pragma omp parallel num_threads(thread_count)trap(a,b,n,&amp;global_result);void trap(double a,double b,int n,double*global_result_p){    double h,x,my_result;    double local_a,local_b;    int local_n;    int my_rank=omp_get_thread_num();    int thread_count=omp_get_num_threads();    h=(b-a)/n;    local_n=n/thread_count;    local_a=a+my_rank*local_n*h;    local_b=local_a+local_n*h;    my_result=(f(local_a)+f(local_b))/2.0;    //先局部求和，避免过多通信    for(int i=1;i&lt;=local_n-1;i++){        x=local_a+i*h;        my_result+=f(x);    }    my_result=my_result*h;    //用临界区保证全局求和正确性    #pragma omp critical        *global_result_p+=my_result;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h3><h4 id="不同任务规模下三种方法执行时间对比"><a href="#不同任务规模下三种方法执行时间对比" class="headerlink" title="不同任务规模下三种方法执行时间对比"></a>不同任务规模下三种方法执行时间对比</h4><p>待计算函数为 f(x) = 1；待计算区间为0到5000。将积分区域分别划分为100，1000，10000，100000个小梯形，比较3种方法在这些不同的任务规模下执行效率的差异。</p><h4 id="计时方法"><a href="#计时方法" class="headerlink" title="计时方法"></a>计时方法</h4><ul><li><p>MPI版本使用的计时方法如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">start = clock();end = clock();time = (end - start) / CLOCKS_PER_SEC;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>Pthread和OpenMP版本继续沿用之前实验中用到的计时方法</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">QueryPerformanceFrequency((LARGE_INTEGER *)&amp;freq);QueryPerformanceCounter((LARGE_INTEGER *)&amp;head);          QueryPerformanceCounter((LARGE_INTEGER *)&amp;tail);  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><h3 id="不同规模下3种方法的结果展示与对比"><a href="#不同规模下3种方法的结果展示与对比" class="headerlink" title="不同规模下3种方法的结果展示与对比"></a>不同规模下3种方法的结果展示与对比</h3><h4 id="程序输出结果"><a href="#程序输出结果" class="headerlink" title="程序输出结果"></a>程序输出结果</h4><p>待计算函数为 f(x) = 1；待计算区间为0到5000</p><ol><li><p>将MPI的运行环境及程序部署到华为鲲鹏云服务器上，得到示例输出如下：</p><p>![image-20230102211323833](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230102211323833.png)</p></li><li><p>编辑器为codeblocks，编译器为mingw64，得到Pthread版本和OpenMP版本的示例输出如下：</p><p>![image-20230102193907704](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230102193907704.png)</p><p>从上述截图中看出，三种实现方式均按照预期效果输出，程序正确。</p></li></ol><h4 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h4><p>按照上述的实验设计执行程序，得到经过分析处理后的图表如下：</p><table><thead><tr><th></th><th>100</th><th>1000</th><th>10000</th><th>100000</th></tr></thead><tbody><tr><td>MPI</td><td>4ms</td><td>5ms</td><td>11ms</td><td>13ms</td></tr><tr><td>Pthread</td><td>13ms</td><td>15ms</td><td>16ms</td><td>19ms</td></tr><tr><td>OpenMP</td><td>7ms</td><td>9ms</td><td>12ms</td><td>14ms</td></tr></tbody></table><p>![image-20230102211927911](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230102211927911.png)</p><p>根据上述图表可以看出，几种实现方式的性能差异并不明显，可能是因为该计算任务本身复杂度较低，三种方式均能快速完成，故无法很好地展现出不同方法的性能差异。单从本实验的表现来看，MPI的执行效率相对来说更高一点，可能是因为多台主机多个进程多个CPU确实会更快一些。当然，这种性能上的优秀表现仅在平均值中体现（上述图表中的数据是多次实验取平均后得到的结果），在某些情况下，MPI方法的执行时间也会出现较长的现象，猜想可能是因为使用云服务器传输数据的过程耗时较长导致。</p><h3 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h3><p>通过本实验，我熟悉并掌握MPI编程方法，在探讨不同规模对不同实现方式的影响，我了解到了多台主机共同完成同一计算任务的思想以及在本实验中的优秀表现。不过本实验中MPI、Pthread、OpenMP三种方法的性能差异并不是特别明显，这也是本实验可以继续改进优化的一个方向。</p><h2 id="实验二：数组排序的任务分配不均衡案例复现"><a href="#实验二：数组排序的任务分配不均衡案例复现" class="headerlink" title="实验二：数组排序的任务分配不均衡案例复现"></a>实验二：数组排序的任务分配不均衡案例复现</h2><h3 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h3><p>对ARR_NUM个长度为ARR_LEN的一维数组进行排序，使用MPI编程，每个进程处理一部分数组排序的任务。若数组分布不均衡，如前二分之一的数组全部升序，后二分之一的数组全部逆序，则每个进程分得的任务负载可能也会不均衡。</p><h3 id="程序设计与实现-1"><a href="#程序设计与实现-1" class="headerlink" title="程序设计与实现"></a>程序设计与实现</h3><p>实验共用到了3台主机的进程完成数组排序。根据进程号连续划分任务，每个进程完成排序之后，将自己执行任务的耗时传送给0号进程，最终0号进程接受各个进程完成排序任务的时间并输出。具体的实现代码如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;iostream&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;#include &lt;mpi.h&gt;using namespace std;//数组数量const int ARR_NUM = 3000;//每个数组的长度const int ARR_LEN = 300;//进程数量/主机数量const int PROCESS_NUM = 3;//每个进程处理的数组数量const int seg = ARR_NUM / PROCESS_NUM;vector&lt;int&gt; arr[ARR_NUM];// 初始化待排序数组，使得// 第一段：完全逆序// 第二段：1/2逆序，1/2升序// 第三段：1/4逆序，3/4升序// 第四段：完全升序void init(void) {    int ratio;    srand(unsigned(time(nullptr)));    for (int i = 0; i &lt; ARR_NUM; i++) {        arr[i].resize(ARR_LEN);        if (i &lt; seg)ratio = 0;        else if (i &lt; seg * 2)ratio = 32;        else ratio = 128;        if ((rand() &amp; 127) &lt; ratio) {            for (int j = 0; j &lt; ARR_LEN; j++) {                arr[i][j] = ARR_LEN - j;            }        }        else {            for (int j = 0; j &lt; ARR_LEN; j++) {                arr[i][j] = j;            }        }    }}//主函数int main() {    int my_rank, comm_sz;    clock_t start, end;    double time;    //初始化数组    init();    MPI_Init(NULL, NULL);    MPI_Comm_rank(MPI_COMM_WORLD, &amp;my_rank);    MPI_Comm_size(MPI_COMM_WORLD, &amp;comm_sz);    //根据进程号分配任务    int startTask = my_rank * seg;    int endTask = startTask + seg;    //排序    start = clock();    for (int i = startTask; i &lt; endTask; i++) {        sort(arr[i].begin(), arr[i].end());    }    end = clock();    time = (end - start);    //其他进程将本进程的运行时间发送给0号进程    if (my_rank != 0) {        MPI_Send(&amp;time, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);    }    //0号进程打印各个进程的运行时间    else if (my_rank == 0) {        printf("各进程运行时间如下:\n");        printf("0号进程: %fms\n", time);        for (int source = 1; source &lt; comm_sz; source++) {            MPI_Recv(&amp;time, 1, MPI_DOUBLE, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);            printf("%d号进程: %fms\n", source, time);        }    }    MPI_Finalize();    return 0;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="实验设计-1"><a href="#实验设计-1" class="headerlink" title="实验设计"></a>实验设计</h3><ul><li>使用init函数生成分布不均的数组；</li><li>使用与实验一（梯形积分法）中相同的计时方法。</li><li>改变数组规模，分别处理500，1000，1500，2000，2500，3000个长度为300的数组，比较在不同规模下各个进程的时间差异，从而理解任务负载不均衡对效率的影响。</li></ul><h3 id="结果展示与对比分析"><a href="#结果展示与对比分析" class="headerlink" title="结果展示与对比分析"></a>结果展示与对比分析</h3><h4 id="程序输出结果-1"><a href="#程序输出结果-1" class="headerlink" title="程序输出结果"></a>程序输出结果</h4><p>将MPI的运行环境及程序部署到华为鲲鹏云服务器上，得到示例输出如下：</p><p>注：本实验给每台主机分配了1个进程（在config文件中设置），总共有3个进程共同执行排序任务，故执行命令时为“3”</p><p>![截屏2023-01-02 21.32.52](/Users/zhangxiaoni/Desktop/截屏2023-01-02 21.32.52.png)</p><h4 id="图表结果分析"><a href="#图表结果分析" class="headerlink" title="图表结果分析"></a>图表结果分析</h4><p>经过分析处理后得到的图表结果展示如下：</p><table><thead><tr><th></th><th>500</th><th>1000</th><th>1500</th><th>2000</th><th>2500</th><th>3000</th></tr></thead><tbody><tr><td>0号进程</td><td>5240ms</td><td>10564ms</td><td>15789ms</td><td>21170ms</td><td>26619ms</td><td>31875ms</td></tr><tr><td>1号进程</td><td>5095ms</td><td>10224ms</td><td>15539ms</td><td>20340ms</td><td>25586ms</td><td>30774ms</td></tr><tr><td>2号进程</td><td>4712ms</td><td>9346ms</td><td>13974ms</td><td>18574ms</td><td>23425ms</td><td>28067ms</td></tr></tbody></table><p> ![image-20230102214239527](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230102214239527.png)</p><p>从上述图表中，我们大致可以看出，由于数据分布不均衡，各个进程的执行时间存在差异，具体表现为：0号、1号、2号进程的执行效率依次增大，这与他们各自的任务负载是保持一致的。在实际情况中，这些负载任务复杂度较高、执行时间较长的进程，会拖慢任务解决的整体时长，这是我们不希望看到的。</p><h3 id="实验总结-1"><a href="#实验总结-1" class="headerlink" title="实验总结"></a>实验总结</h3><p>​通过本实验，我了解到数据分布均衡对于多进程、多线程程序的重要性。如果任务分布不均，将会出现执行效率较低的进程拖慢任务解决的整体效率的情况。因此，并行编程时控制任务规模、控制任务分配的粗细粒度、控制进程或线程的数量，使得程序在数据分布不均衡的情况下也能有较为优秀的表现就显得很重要。当然，也可以在程序执行任务之前，对数据进行处理，达到各个进程分配的数据分布都较为均衡的效果。</p><h2 id="附加实验：高斯消去法解线性方程组"><a href="#附加实验：高斯消去法解线性方程组" class="headerlink" title="附加实验：高斯消去法解线性方程组"></a>附加实验：高斯消去法解线性方程组</h2><h3 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h3><p>对于给定的线性方程组Ax=b，使用MPI编程，完成对于线性方程组的高斯消元。</p><h3 id="程序设计与实现-2"><a href="#程序设计与实现-2" class="headerlink" title="程序设计与实现"></a>程序设计与实现</h3><p>每个进程完成部分行的消元计算任务，然后使用barrier功能，使得所有进程均完成第k行的消元后再计算下一行的消元。程序实现的具体代码如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;stdio.h&gt;#include&lt;iostream&gt;#include &lt;algorithm&gt;#include&lt;cstdlib&gt;#include&lt;ctime&gt;#include&lt;mpi.h&gt;using namespace std;const int n = 900;const int NUM_PROC = 3;float a[n][n], b[n], c[n];void init() {    for (int i = 0; i &lt; n; i++) {        for (int j = 0; j &lt; n; j++) {            a[i][j] = rand() % (1000 - 1) + 1;        }        b[i] = rand() % (1000 - 1) + 1;    }}int main() {    int my_rank, comm_sz, left, right;    int source;    clock_t start, finish;    double sec;    init();    MPI_Init(NULL, NULL);    MPI_Comm_rank(MPI_COMM_WORLD, &amp;my_rank);    MPI_Comm_size(MPI_COMM_WORLD, &amp;comm_sz);    start = clock();    for (int k = 0; k &lt; n - 1; k++) {        int seg = (n - (k + 1)) / NUM_PROC;        left = my_rank * seg;        right = (my_rank + 1) * seg;        for (int i = left; i &lt; right; i++) {            c[i] = a[i][k] / a[k][k];        }        for (int i = left; i &lt; right; i++) {            for (int j = 0; j &lt; n; j++) {                a[i][j] = a[i][j] - c[i] * a[k][j];            }            b[i] = b[i] - c[i] * b[k];        }        MPI_Barrier(MPI_COMM_WORLD);    }    finish = clock();    sec = finish - start;    if (my_rank != 0) {        MPI_Send(&amp;sec, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);    }    else {        printf("0号进程: %f\n", sec);        for (source = 1; source &lt; comm_sz; source++) {            MPI_Recv(&amp;sec, 1, MPI_DOUBLE, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);            printf("%d号进程: %f\n", source, sec);        }    }    MPI_Finalize();    return 0;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="实验总结-2"><a href="#实验总结-2" class="headerlink" title="实验总结"></a>实验总结</h3><p>该部分尝试对高斯消元法进行了的MPI的编程实现，其中也遇到了很多困难，比如数据依赖等问题。经过反复尝试，我实现了高斯消元MPI的解法，使得我对于MPI编程中需要注意的内存共享的问题有了更深刻的理解。</p>]]></content>
      
      
      <categories>
          
          <category> 并行程序设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并行程序设计 </tag>
            
            <tag> S5课上 </tag>
            
            <tag> 实验报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并行程序设计-第六讲.MPI编程</title>
      <link href="/2022/11/13/di-liu-jiang-mpi-bian-cheng/"/>
      <url>/2022/11/13/di-liu-jiang-mpi-bian-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="第六讲-MPI编程"><a href="#第六讲-MPI编程" class="headerlink" title="第六讲 MPI编程"></a>第六讲 MPI编程</h1><h2 id="MPI概念和基本原语"><a href="#MPI概念和基本原语" class="headerlink" title="MPI概念和基本原语"></a>MPI概念和基本原语</h2><h3 id="消息传递和MPI"><a href="#消息传递和MPI" class="headerlink" title="消息传递和MPI"></a>消息传递和MPI</h3><ul><li>消息传递是超级计算机和集群主要的编程模型</li><li>MPI是什么<ul><li>消息传递编程模型标准，取代专有库</li><li>编程角度</li><li>基于单程序多数据流（SPMD）</li><li>隔离了独立地址空间<ul><li>不会有数据竞争，但可能有通信错误</li></ul></li></ul></li></ul><h3 id="消息传递库特性"><a href="#消息传递库特性" class="headerlink" title="消息传递库特性"></a>消息传递库特性</h3><ul><li><p>所有通信、同步都需调用函数完成</p><ul><li><strong>无共享变量</strong></li></ul></li><li><p>提供如下类别的函数</p><ul><li><p>通信</p><ul><li>点对点通信：消息从特性的发送<strong>进程</strong>（点A）发送到特定的接收进程（点B）</li><li>多处理器参与的组通信<ul><li>移动数据：广播、散发/收集</li><li>计算并移动数据：归约、全归约</li></ul></li></ul></li><li><p>同步</p><ul><li>障碍</li><li>无锁机制，因为没有共享变量需要保护</li></ul></li><li><p>查询</p><ul><li>多少个进程？哪个是我？有处于等待状态的信息？</li></ul></li></ul></li></ul><h3 id="基本接口"><a href="#基本接口" class="headerlink" title="基本接口"></a>基本接口</h3><ul><li>MPI_Comm_size报告进程数<ul><li>int MPI_Comm_size(MPI_Comm comm,int *size)</li></ul></li><li>MPI_Comm_rank报告识别调用进程的rank，值从0～size-1<ul><li>int MPI_Comm_rank(MPI_Comm comm,int *rank)</li></ul></li></ul><h4 id="编译–Linux平台"><a href="#编译–Linux平台" class="headerlink" title="编译–Linux平台"></a>编译–Linux平台</h4><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221121144656484.png" alt="image-20221121144656484"></p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221121144808070.png" alt="image-20221121144808070"></p><h4 id="运行MPI程序"><a href="#运行MPI程序" class="headerlink" title="运行MPI程序"></a>运行MPI程序</h4><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221121145434261.png" alt="image-20221121145434261"></p><h4 id="MPI初始化和结束处理"><a href="#MPI初始化和结束处理" class="headerlink" title="MPI初始化和结束处理"></a>MPI初始化和结束处理</h4><ul><li><p>MPI_Init</p><ul><li>令MPI进行必要的初始化工作</li><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221121145615335.png" alt="image-20221121145615335"></li></ul></li><li><p>MPI_Finalize</p><ul><li>告诉MPI程序已结束，进行清理工作</li><li>int MPI_Finalize(void)</li></ul></li></ul><h4 id="MPI程序基本结构"><a href="#MPI程序基本结构" class="headerlink" title="MPI程序基本结构"></a>MPI程序基本结构</h4><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221121145951697.png" alt="image-20221121145951697"></p><h3 id="MPI消息传递"><a href="#MPI消息传递" class="headerlink" title="MPI消息传递"></a>MPI消息传递</h3><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221121150207143.png" alt="image-20221121150207143"></p><ul><li>消息传递最基本的函数：<ul><li>send <ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221121151703266.png" alt="image-20221121151703266"></li></ul></li><li>receive<ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221121151832262.png" alt="image-20221121151832262"></li></ul></li></ul></li><li><h2 id="需要明确-如何描述数据-MPI强数据类型-在传输之前必须指明数据是什么类型、数据个数等信息-如何标识进程-rank-接收方如何识别信息"><a href="#需要明确-如何描述数据-MPI强数据类型-在传输之前必须指明数据是什么类型、数据个数等信息-如何标识进程-rank-接收方如何识别信息" class="headerlink" title="需要明确- 如何描述数据  - MPI强数据类型  - 在传输之前必须指明数据是什么类型、数据个数等信息- 如何标识进程  - rank- 接收方如何识别信息"></a>需要明确<br>- 如何描述数据<br>  - MPI强数据类型<br>  - 在传输之前必须指明数据是什么类型、数据个数等信息<br>- 如何标识进程<br>  - rank<br>- 接收方如何识别信息</h2><ul><li>操作完成意味着什么</li></ul></li></ul><h3 id="一些基本概念"><a href="#一些基本概念" class="headerlink" title="一些基本概念"></a>一些基本概念</h3><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221121152126455.png" alt="image-20221121152126455"></p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221121152225212.png" alt="image-20221121152225212"></p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221121152446813.png" alt="image-20221121152446813"></p><p>这块看pppt</p><h2 id="MPI编程模型"><a href="#MPI编程模型" class="headerlink" title="MPI编程模型"></a>MPI编程模型</h2>]]></content>
      
      
      <categories>
          
          <category> 并行程序设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并行程序设计 </tag>
            
            <tag> S5课上 </tag>
            
            <tag> 知识总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并行程序设计-实验4.OpenMP编程练习</title>
      <link href="/2022/10/22/openmp-bian-cheng-lian-xi/"/>
      <url>/2022/10/22/openmp-bian-cheng-lian-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="OpenMP-编程练习"><a href="#OpenMP-编程练习" class="headerlink" title="OpenMP 编程练习"></a>OpenMP 编程练习</h1><h2 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h2><ol><li>分别实现课件中的梯形积分法的 Pthread、OpenMP 版本， 熟悉并掌握 OpenMP 编程方法，探讨两种编程方式的异同。</li><li>对于课件中“多个数组排序”的任务不均衡案例进行 OpenMP 编程实现（规模可自己调整），并探索不同循环调度方案的优劣。提示：可从任务分块的大小、线程数的多少、静态动态多线程结合等方面进行尝试，探索规律。</li><li>附加题：实现高斯消去法解线性方程组的 OpenMP 编程，与 SSE/AVX 编程结合，并探索优化任务分配方法。</li></ol><h2 id="梯形积分法"><a href="#梯形积分法" class="headerlink" title="梯形积分法"></a>梯形积分法</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>​对于下图中给出的梯形积分法，实现多线程编程。每个线程实现 a 到 b 区间中的某一段的梯形积分的计算。</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221204172704931.png" alt="image-20221204172704931"></p><h3 id="算法设计与实现"><a href="#算法设计与实现" class="headerlink" title="算法设计与实现"></a>算法设计与实现</h3><h4 id="使用Pthread实现"><a href="#使用Pthread实现" class="headerlink" title="使用Pthread实现"></a>使用Pthread实现</h4><p>​对于每个线程，先求局部和最后全局求和，避免过多通信。同时注意给全局和加锁保证结果的正确性。代码实现如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void calculate_differential(int i) {    double x = a + i * h;    double currArea = func(x) * h;    pthread_mutex_lock(&amp;barrier_mutex);    totalPthread += currArea;    pthread_mutex_unlock(&amp;barrier_mutex);}void *Cal_pthread(void *parm) {    int task = 0;    while (true) {        pthread_mutex_lock(&amp;barrier_mutex);        next_task += seg;        task = next_task;        pthread_mutex_unlock(&amp;barrier_mutex);        if (task &gt;= n + seg)            break;        else {            for (int i = task - seg; i &lt; (task &lt; n ? task : n); i++) {                calculate_differential(i);            }        }    }    pthread_exit(nullptr);}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="使用OpenMP实现"><a href="#使用OpenMP实现" class="headerlink" title="使用OpenMP实现"></a>使用OpenMP实现</h4><p>​利用 OpenMP 的相关方法实现多线程并行编程，思路与 Pthread 编程类似。代码实现如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void Trap(double a, double b, double h, int n, double *global_result_p) {    double x, my_result;    double local_a, local_b;    int i, local_n;    int my_rank = omp_get_thread_num();    int thread_count = omp_get_num_threads();    local_n = n / thread_count;    local_a = a + my_rank * local_n * h;    local_b = local_a + local_n * h;    my_result = (func(local_a) + func(local_b)) / 2.0;    for (i = 1; i &lt;= local_n; i++) {        x = local_a + i * h;        my_result += func(x);    }    my_result = my_result * h;# pragma omp critical    *global_result_p += my_result;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h3><p>​将待积分的函数设置为2*x^2-x，积分区间从2到10，将整个图形划分为2000个小梯形。根据实验结果，可以看出多线程和OpenMP实验结果相同，梯形面积都为613.333。</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221204211530816.png" alt="image-20221204211530816"></p><h3 id="Pthread-和-OpenMP-两种编程方式异同比较"><a href="#Pthread-和-OpenMP-两种编程方式异同比较" class="headerlink" title="Pthread 和 OpenMP 两种编程方式异同比较"></a>Pthread 和 OpenMP 两种编程方式异同比较</h3><ol><li>Pthread 在程序启动时创建线程，再将工作分配到线程上。然而，这种方法需要相当多的线程指定代码，而且不能保证能够随着可用处理器的数量而合理地进行扩充。OpenMP 不需要指定数量，在有循环的地方加上代码，修改设置文件即可。OpenMP 非常方便，因为它不会将软件锁定在事先设定的线程数量中，但是相对的查错更难也更麻烦。 </li><li>OpenMP 和 Pthread 之间的区别主要在编译的方式上，OpenMP 的编译需要添加编译器预 处理指令#pragma，创建线程等后续工作要编译器来完成。而 pthread 就是一个库，所有的 并行线程创建都需要我们自己完成，较 OpenMP 麻烦一点。但如果开发人员需要精细纹理的控制，Pthread 能够提供更大范围的原函数，属于更优的选择。 </li><li>OpenMP 的编译指示还有另一项重要优势：通过禁用 OpenMP 支持，代码可用作为单 一线程应用进行编译。当调试程序时，以这样的方式编译代码拥有巨大优势。如果没有这种选择，开发人员会经常发现很难说明复杂的代码是否能够正确工作，因为线程问题或因为与线程无关的设计错误。</li></ol><h2 id="数组排序的任务分配不均衡案例复现"><a href="#数组排序的任务分配不均衡案例复现" class="headerlink" title="数组排序的任务分配不均衡案例复现"></a>数组排序的任务分配不均衡案例复现</h2><h3 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h3><p>​对 ARR_NUM 个长度为 ARR_LEN 的一维数组进行排序，使用 OpenMP 多线程编程，每个线程处理一部分数组。若数组分布不均衡，如前二分之一的数组全部升序，后二分之一的数组全部逆序，则每个线程分得的任务负载可能也会不均衡。</p><h3 id="算法设计与实现-1"><a href="#算法设计与实现-1" class="headerlink" title="算法设计与实现"></a>算法设计与实现</h3><h4 id="均匀随机初始化数组"><a href="#均匀随机初始化数组" class="headerlink" title="均匀随机初始化数组"></a>均匀随机初始化数组</h4><p>​完全随机生成的各个数组内数据顺序完全随机，基本无差异。将数组列数设置为 2000，行数设置为 2000，线程数设置为 4，生成的数据大小范围在 0~10000 之间。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void init_uniform(void) {    srand(unsigned(time(nullptr)));    for (int i = 0; i &lt; ARR_NUM; i++) {        for (int j = 0; j &lt; ARR_LEN; j++)            arr[i][j] = rand() % 10000;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="不均匀随机初始化数组"><a href="#不均匀随机初始化数组" class="headerlink" title="不均匀随机初始化数组"></a>不均匀随机初始化数组</h4><p>​不均匀随机初始化待排序数组，使得第一段数组完全逆序，第二段数组中 1/2 逆序，1/2 升序，第三段数组中 1/4 逆序，3/4 升序，第四段数组中完全升序。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">// 不均匀初始化数组void init_uneven(void){    int ratio;    srand(unsigned(time(nullptr)));    for (int i = 0; i &lt; ARR_NUM; i++){        if(i &lt; seg) ratio = 0;        else if(i &lt; seg * 2) ratio = 32;        else if(i &lt; seg * 3) ratio = 64;        else ratio = 128;        if((rand() &amp; 127) &lt; ratio){            for(int j = 0; j &lt; ARR_LEN; j++)                arr[i][j] = ARR_LEN - j;        } else{            for(int j = 0; j &lt; ARR_LEN; j++)                arr[i][j] = j;        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="使用OpenMP对某块数组进行排序"><a href="#使用OpenMP对某块数组进行排序" class="headerlink" title="使用OpenMP对某块数组进行排序"></a>使用OpenMP对某块数组进行排序</h4><p>​指定task = next_arr += seg;在同一时间只能被一条线程执行，每次对seg块的数组进行排序。完成后线程再次领取新的任务，直至所有数组均已排序完成。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">// OpenMP排序void OpenMP_sort() {    int task = 0;    while (true) {#pragma omp critical        task = next_arr += seg;        if (task &gt;= ARR_NUM + seg)            break;        for (int i = task - seg; i &lt; (task &lt; ARR_NUM ? task : ARR_NUM); i++)            sort_bubbling(arr[i]);    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="控制粒度粗细"><a href="#控制粒度粗细" class="headerlink" title="控制粒度粗细"></a>控制粒度粗细</h4><p>​通过改变seg的大小，控制排序时每个线程分得的行数大小，从而改变粒度粗细。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">    // 粗细粒度变化    THREAD_NUM = 4;    init_uniform();    rollover(arr, arrtemp);    for (seg = 10; seg &lt;= 100; seg += 10) {        rollover(arrtemp, arr);        next_arr = 0;        gettimeofday(&amp;startTime, NULL);#pragma omp parallel num_threads(THREAD_NUM)        OpenMP_sort();        gettimeofday(&amp;stopTime, NULL);    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="实验结果分析-1"><a href="#实验结果分析-1" class="headerlink" title="实验结果分析"></a>实验结果分析</h3><h4 id="均匀随机分布的数组排序"><a href="#均匀随机分布的数组排序" class="headerlink" title="均匀随机分布的数组排序"></a>均匀随机分布的数组排序</h4><h5 id="粒度变化"><a href="#粒度变化" class="headerlink" title="粒度变化"></a>粒度变化</h5><p>​程序输出样例如下：</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221204210335474.png" alt="image-20221204210335474"></p><p>​经过分析处理后得到的图表结果展示如下：</p><table><thead><tr><th>seg</th><th>10</th><th>20</th><th>30</th><th>40</th><th>50</th><th>60</th><th>70</th><th>80</th><th>90</th><th>100</th></tr></thead><tbody><tr><td>time</td><td>2234.3</td><td>2212.9</td><td>2208.4</td><td>2252.2</td><td>2202.4</td><td>2341.8</td><td>2290.4</td><td>2442.8</td><td>2324.2</td><td>2193.8</td></tr></tbody></table><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221204210451419.png" alt="image-20221204210451419"></p><p>​从上述实验结果可以看出，在数组完全随机，数组内数据分布均匀的情况下，粒度从40增加到90时，耗时波动起伏较大；在粒度大小为80时，耗时达到峰值。同时可以看出，在粒度大小等于50时，耗时处于低谷。</p><p>​证明在数组均匀随机分布的情况下，粒度大小的变化会对耗时产生较大影响，选择合适粒度能够显著提高排序速度。</p><h5 id="线程变化"><a href="#线程变化" class="headerlink" title="线程变化"></a>线程变化</h5><p>​程序输出样例如下：</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221204210323877.png" alt="image-20221204210323877"></p><p>​经过分析处理后得到的图表结果展示如下：</p><table><thead><tr><th>thread</th><th>2</th><th>4</th><th>6</th><th>8</th><th>10</th><th>12</th><th>14</th><th>16</th><th>18</th><th>20</th></tr></thead><tbody><tr><td>time</td><td>4206.8</td><td>2253.7</td><td>1551.8</td><td>1216.4</td><td>1045.5</td><td>909.5</td><td>824.9</td><td>784.9</td><td>791.1</td><td>788.9</td></tr></tbody></table><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221204210121608.png" alt="image-20221204210121608"></p><p>​从实验结果可以看出，在数组均匀随机分布的情况下，随着线程数增加，实验耗时逐渐减小。当线程数大于12时，耗时趋于平缓。证明在线程数等于12时，排序速度已经达到较高水平，无需再增加线程数。</p><h4 id="不均匀随机分布的数组排序"><a href="#不均匀随机分布的数组排序" class="headerlink" title="不均匀随机分布的数组排序"></a>不均匀随机分布的数组排序</h4><h5 id="粒度变化-1"><a href="#粒度变化-1" class="headerlink" title="粒度变化"></a>粒度变化</h5><p>​程序输出样例如下：</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221204210400115.png" alt="image-20221204210400115"></p><p>​经过分析处理后得到的图表结果展示如下：</p><table><thead><tr><th>seg</th><th>10</th><th>20</th><th>30</th><th>40</th><th>50</th><th>60</th><th>70</th><th>80</th><th>90</th><th>100</th></tr></thead><tbody><tr><td>time</td><td>2221.5</td><td>2235.3</td><td>2211</td><td>2277.9</td><td>2229.2</td><td>2340.2</td><td>2324.2</td><td>2382.6</td><td>2358.1</td><td>2219.2</td></tr></tbody></table><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221204210656780.png" alt="image-20221204210656780"></p><p>​从上述实验结果可以看出，在数组完全随机，数组内数据分布不均匀的情况下，粒度从40增加到100时，耗时波动起伏较大；在粒度大小为80时，耗时达到峰值。同时可以看出，在粒度大小等于30时，耗时处于低谷。</p><p>​证明在数组不均匀随机分布的情况下，粒度大小的变化会对耗时产生较大影响，选择合适粒度能够显著提高排序速度。</p><h5 id="线程变化-1"><a href="#线程变化-1" class="headerlink" title="线程变化"></a>线程变化</h5><p>​程序输出样例如下：</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221204210352484.png" alt="image-20221204210352484"></p><p>​经过分析处理后得到的图表结果展示如下：</p><table><thead><tr><th>thread</th><th>2</th><th>4</th><th>6</th><th>8</th><th>10</th><th>12</th><th>14</th><th>16</th><th>18</th><th>20</th></tr></thead><tbody><tr><td>time</td><td>1096.3</td><td>638.9</td><td>448.5</td><td>411.3</td><td>359.3</td><td>345.8</td><td>335.9</td><td>337.1</td><td>279.9</td><td>278.8</td></tr></tbody></table><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221204210638549.png" alt="image-20221204210638549"></p><p>​从实验结果可以看出，在数组不均匀随机分布的情况下，随着线程数增加，实验耗时逐渐减小。当线程数大于6时，耗时趋于平缓。证明在线程数等于6时，排序速度已经达到较高水平，无需再增加线程数。</p><h2 id="高斯消去法解线性方程组的-OpenMP-多线程编程实现"><a href="#高斯消去法解线性方程组的-OpenMP-多线程编程实现" class="headerlink" title="高斯消去法解线性方程组的 OpenMP 多线程编程实现"></a>高斯消去法解线性方程组的 OpenMP 多线程编程实现</h2><h3 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h3><p>​对于给定的方程组 Ax=b，使用 OpenMP多线程编程，同时结合 SSE 向量计算，完成对于线性方程组的高斯消元和回代求解。本实验采取静态划分、动态划分、粗粒度动态划分多个角 度，探索任务分块大小、线程数多少、静态动态多线程结合等方面因素对高斯消去法解线性方程组效率的影响。并探索多线程分配任务的最优方案。</p><h3 id="算法设计与实现-2"><a href="#算法设计与实现-2" class="headerlink" title="算法设计与实现"></a>算法设计与实现</h3><h4 id="使用OpenMP"><a href="#使用OpenMP" class="headerlink" title="使用OpenMP"></a>使用OpenMP</h4><p>​使用SSE，以4个元素为一个单位进行消元，剩余最后三个元素串行处理。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">// s使用SSE对j行消元void SSE_elimination(int i, int j) {    float tep;    __m128 div, t1, t2, sub;    tep = a[j][i] / a[i][i];    div = _mm_set1_ps(tep);    // 每次处理4个元素    int k;    for (k = n - 3; k &gt;= i + 1; k -= 4) {        t1 = _mm_loadu_ps(a[i] + k);        t2 = _mm_loadu_ps(a[j] + k);        sub = _mm_sub_ps(t2, _mm_mul_ps(t1, div));        _mm_storeu_ps(a[j] + k, sub);    }    for (k += 3; k &gt;= i + 1; k--) {        a[j][k] -= a[i][k] * tep;    }    a[j][i] = 0.00;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​使用 OpenMP 的提供的相关方法，实现高斯消元。并结合 sse 指令向量化求解，提高效率。 具体实现代码如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void OpenMP_func(int next_task) {    int task = 0;    while (true) {#pragma omp critical        {            task = next_task;            next_task += seg;        }        if (task &gt;= n)            break;        else {            for (int i = task; i &lt; (task + seg &lt; n ? task + seg : n); i++)                SSE_elimination(line, i);        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="实验结果分析-2"><a href="#实验结果分析-2" class="headerlink" title="实验结果分析"></a>实验结果分析</h3><h4 id="线程变化-2"><a href="#线程变化-2" class="headerlink" title="线程变化"></a>线程变化</h4><p>​程序输出样例如下：</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221204204651302.png" alt="image-20221204204651302"></p><p>​经过分析处理后得到的图表结果展示如下：</p><table><thead><tr><th>thread</th><th>2</th><th>4</th><th>6</th><th>8</th><th>10</th><th>12</th><th>14</th><th>16</th><th>18</th><th>20</th></tr></thead><tbody><tr><td>time</td><td>123.8</td><td>154</td><td>185.7</td><td>232</td><td>281.3</td><td>321.7</td><td>356</td><td>390.1</td><td>435</td><td>483.1</td></tr></tbody></table><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221204215002447.png" alt="image-20221204215002447"></p><p>​根据以上实验结果，可以看出，实验耗时随着线程数增加而增加，有违理论基础。可能是由于随着线程数增加，不必要的开销增多，执行效率降低。</p><h4 id="粒度变化-2"><a href="#粒度变化-2" class="headerlink" title="粒度变化"></a>粒度变化</h4><p>​程序输出样例如下：</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221204204701976.png" alt="image-20221204204701976"></p><p>​经过分析处理后得到的图表结果展示如下：</p><table><thead><tr><th>seg</th><th>10</th><th>20</th><th>30</th><th>40</th><th>50</th><th>60</th><th>70</th><th>80</th><th>90</th><th>100</th></tr></thead><tbody><tr><td>time</td><td>162.293</td><td>149.935</td><td>147.02</td><td>147.935</td><td>149.626</td><td>151.116</td><td>151.101</td><td>146.928</td><td>147.225</td><td>146.675</td></tr></tbody></table><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221204215102341.png" alt="image-20221204215102341"></p><p>​从上述实验结果可以看出，粒度从30增加到100时，耗时波动起伏较小；在粒度大小为10时，耗时达到峰值。同时可以看出，在粒度大小等于80时，耗时处于低谷。</p><p>​证明粒度大小的变化会对高斯消去法解线性方程组实验耗时产生较大影响，选择合适粒度能够显著提高消元速度。</p>]]></content>
      
      
      <categories>
          
          <category> 并行程序设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并行程序设计 </tag>
            
            <tag> S5课上 </tag>
            
            <tag> 实验报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并行程序设计-第五讲.openmp编程</title>
      <link href="/2022/10/18/di-wu-jiang-openmp/"/>
      <url>/2022/10/18/di-wu-jiang-openmp/</url>
      
        <content type="html"><![CDATA[<h1 id="第五讲-openmp"><a href="#第五讲-openmp" class="headerlink" title="第五讲 openmp"></a>第五讲 openmp</h1><h2 id="OpenMP并行模型"><a href="#OpenMP并行模型" class="headerlink" title="OpenMP并行模型"></a>OpenMP并行模型</h2><h3 id="程序员视角"><a href="#程序员视角" class="headerlink" title="程序员视角"></a>程序员视角</h3><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114104633837.png" alt="image-20221114104633837"></p><h4 id="执行模型"><a href="#执行模型" class="headerlink" title="执行模型"></a>执行模型</h4><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114104747893.png" alt="image-20221114104747893"></p><h4 id="编程环境配置"><a href="#编程环境配置" class="headerlink" title="编程环境配置"></a>编程环境配置</h4><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114104917033.png" alt="image-20221114104917033"></p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114104938547.png" alt="image-20221114104938547"></p><h3 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h3><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114105212217.png" alt="image-20221114105212217"></p><h4 id="编译指示格式"><a href="#编译指示格式" class="headerlink" title="编译指示格式"></a>编译指示格式</h4><ul><li>编译指示格式<ul><li>#pragma omp directive_name[clause[clause]……]</li></ul></li><li>条件编译<ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114105619319.png" alt="image-20221114105619319"></li><li>如果使用的平台支持openmp则。。</li><li>不支持的话就按照串行方式，保证结果正确</li></ul></li><li>大小写敏感</li><li>使用库函数需要包含头文件</li></ul><h4 id="运行时库，查询函数"><a href="#运行时库，查询函数" class="headerlink" title="运行时库，查询函数"></a>运行时库，查询函数</h4><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114105948585.png" alt="image-20221114105948585"></p><h4 id="并行区域结构"><a href="#并行区域结构" class="headerlink" title="并行区域结构"></a>并行区域结构</h4><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114110155127.png" alt="image-20221114110155127"></p><h4 id="hello-world程序"><a href="#hello-world程序" class="headerlink" title="hello world程序"></a>hello world程序</h4><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;omp.h&gt;void Hello(void);int main(int argc,char* argv[]){  int thread_count=strtol(argv[1],NULL,10);    #pragma omp parallel num_threads(thread_count)  Hello();    return 0;}void Hello(void){  int my_rank=omp_get_thread_num();  int thread_count=omp_get_num_threads();    print("Hello from thread %d of %d\n",my_rank,thread_count);}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>为防止编译器不支持openMP</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114111804810.png" alt="image-20221114111804810"></p><h4 id="梯形积分法"><a href="#梯形积分法" class="headerlink" title="梯形积分法"></a>梯形积分法</h4><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114112046950.png" alt="image-20221114112046950"></p><h4 id="临界区指令"><a href="#临界区指令" class="headerlink" title="临界区指令"></a>临界区指令</h4><p>归约 </p><h2 id="并行循环"><a href="#并行循环" class="headerlink" title="并行循环"></a>并行循环</h2><h3 id="OpenMp数据并行：并行循环"><a href="#OpenMp数据并行：并行循环" class="headerlink" title="OpenMp数据并行：并行循环"></a>OpenMp数据并行：并行循环</h3><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114141348225.png" alt="image-20221114141348225"></p><h3 id="局限和语义"><a href="#局限和语义" class="headerlink" title="局限和语义"></a>局限和语义</h3><p>必须提前看到for循环就能看到有多少任务</p><p>带break、goto之类的不支持</p><p>不支持依赖的（例如斐波那契数列，每次计算都要用到前面的数）</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114142008518.png" alt="image-20221114142008518"></p><h3 id="简单并行化循环的版本"><a href="#简单并行化循环的版本" class="headerlink" title="简单并行化循环的版本"></a>简单并行化循环的版本</h3><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114142232525.png" alt="image-20221114142232525"></p><h3 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h3><p>openmp隐式同步</p><p>在要并行的语句前隐式的有开始并行和join之类的</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114142437576.png" alt="image-20221114142437576"></p><h3 id="并行for指示的各种形式"><a href="#并行for指示的各种形式" class="headerlink" title="并行for指示的各种形式"></a>并行for指示的各种形式</h3><h2 id="数据依赖（中间跳过了一部分，只讲了下面的几个ppt，跳过p39、）"><a href="#数据依赖（中间跳过了一部分，只讲了下面的几个ppt，跳过p39、）" class="headerlink" title="数据依赖（中间跳过了一部分，只讲了下面的几个ppt，跳过p39、）"></a>数据依赖（中间跳过了一部分，只讲了下面的几个ppt，跳过p39、）</h2><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114142745626.png" alt="image-20221114142745626"></p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114143132650.png" alt="image-20221114143132650"></p><p>第一个存在数据依赖</p><p>第二个不存在依赖</p><ul><li><p>例子</p><ul><li><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114143408845.png" alt="image-20221114143408845"></p></li><li><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114143519896.png" alt="image-20221114143519896"></p></li><li><p>气泡排序（🌟）</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114143824119.png" alt="image-20221114143824119"></p><p>内层和外层哪个存在循环依赖？？</p><p>都存在</p><p>外层：每次循环数组的顺序都发生了变化，下一次循环依赖上次循环的结果，存在循环依赖关系</p><p>内层，每次都要比较a[i]和a[i+1],并要交换两者位置，存在循环依赖关系</p></li><li><p>气泡排序内外层都存在依赖，怎么进行并行化</p><ul><li>解决方法</li><li>第一轮奇数项和他的下一个比较，第二轮偶数项和她的下一个比较</li><li>每一轮之间不存在依赖了</li><li>串行的代码<ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114150929703.png" alt="image-20221114150929703"></li></ul></li><li>并行代码<ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114150549415.png" alt="image-20221114150549415"></li><li>有什么问题</li><li>每次大循环的时候频繁地创建线程以及销毁线程，耗费系统资源</li><li>怎么改进？</li><li>把创建线程拿到大循环外面</li><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114150633942.png" alt="image-20221114150633942"></li><li>不用加parrallel_for啥的，因为不是对外循环并行化。只是现在外面生成线程，在执行内循环的时候在写pragma omp for，把任务分到线程上</li></ul></li></ul></li></ul></li></ul><h2 id="循环调度"><a href="#循环调度" class="headerlink" title="循环调度"></a>循环调度</h2><p>什么都不写的话默认static，最大块</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114153015722.png" alt="image-20221114153015722"></p><h3 id="静态分配"><a href="#静态分配" class="headerlink" title="静态分配"></a>静态分配</h3><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114151337679.png" alt="image-20221114151337679"></p><p>不指定的话默认为threads，指定的话变成循环分配，下面是制定了static（2）</p><p>在执行任务之前划分好，执行的时候</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114151712217.png" alt="image-20221114151712217"></p><h3 id="动态分配"><a href="#动态分配" class="headerlink" title="动态分配"></a>动态分配</h3><p>谁先执行好把剩下的最后一块给哪个线程</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114151913079.png" alt="image-20221114151913079"></p><h3 id="guided动态划分"><a href="#guided动态划分" class="headerlink" title="guided动态划分"></a>guided动态划分</h3><p>剩余任务数➗二倍的线程数</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114152516458.png" alt="image-20221114152516458"></p><h3 id="更多属性"><a href="#更多属性" class="headerlink" title="更多属性"></a>更多属性</h3><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114152850597.png" alt="image-20221114152850597"></p><h2 id="局部性"><a href="#局部性" class="headerlink" title="局部性"></a>局部性</h2><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114154109286.png" alt="image-20221114154109286"></p><p>重用</p><p>===========中间跳过了</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114154155141.png" alt="image-20221114154155141"></p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221114154240600.png" alt="image-20221114154240600"></p><p>后面全都跳过了</p><h2 id="任务并行"><a href="#任务并行" class="headerlink" title="任务并行"></a>任务并行</h2>]]></content>
      
      
      <categories>
          
          <category> 并行程序设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并行程序设计 </tag>
            
            <tag> S5课上 </tag>
            
            <tag> 知识总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并行程序设计-第四讲.Pthread编程</title>
      <link href="/2022/10/03/di-si-jiang-duo-xian-cheng-pthread-bian-cheng/"/>
      <url>/2022/10/03/di-si-jiang-duo-xian-cheng-pthread-bian-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="第四讲-多线程Pthread编程"><a href="#第四讲-多线程Pthread编程" class="headerlink" title="第四讲 多线程Pthread编程"></a>第四讲 多线程Pthread编程</h1><h2 id="共享内存系统和分布式内存模型回顾"><a href="#共享内存系统和分布式内存模型回顾" class="headerlink" title="共享内存系统和分布式内存模型回顾"></a>共享内存系统和分布式内存模型回顾</h2><h3 id="伪共享"><a href="#伪共享" class="headerlink" title="伪共享"></a>伪共享</h3><ol><li><p>cache按照行读取</p></li><li><p>当多个处理器访问同一行，即使访问的是不同的机器字，也会潜在竞争</p></li><li><p>会产生不必要的协同开销</p><p><img src="https://tva1.sinaimg.cn/large/008vxvgGly1h7oggjpznoj30m20aiwf1.jpg" alt="image-20221031143000351"></p><ol><li>当数据很少的时候，Core1和Core0访问的是同一行，同一个缓存行里的不同变量在同时被修改</li></ol></li></ol><h3 id="共享内存编程"><a href="#共享内存编程" class="headerlink" title="共享内存编程"></a>共享内存编程</h3><ol><li>动态线程<ol><li>主线程等待计算工作，fork新线程分配工作，工作线程完成任务后结束</li><li>资源利用率高</li></ol></li><li>主线程完成时fork出<strong>所有线程</strong><ol><li>性能更优，但可能浪费系统资源</li></ol></li></ol><h3 id="并行程序设计的复杂性"><a href="#并行程序设计的复杂性" class="headerlink" title="并行程序设计的复杂性"></a>并行程序设计的复杂性</h3><h2 id="POSIX-Threads编程"><a href="#POSIX-Threads编程" class="headerlink" title="POSIX Threads编程"></a>POSIX Threads编程</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>线程库：</p><ul><li>Pthread是POSIX标准<ul><li>相对底层</li><li>可移植</li></ul></li><li>OpenMP是新标准<ul><li>高层编程，适用于共享内存架构上的科学计算</li></ul></li></ul><h4 id="POSIX-Thread"><a href="#POSIX-Thread" class="headerlink" title="POSIX Thread"></a>POSIX Thread</h4><p><img src="https://tva1.sinaimg.cn/large/008vxvgGly1h7oguexqwnj30zo0m8djv.jpg" alt="image-20221031144319854"></p><h3 id="基础API"><a href="#基础API" class="headerlink" title="基础API"></a>基础API</h3><h4 id="创建线程"><a href="#创建线程" class="headerlink" title="创建线程"></a>创建线程</h4><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">int</span> <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token class-name">pthread_t</span><span class="token operator">*</span><span class="token punctuation">,</span><span class="token keyword">const</span> <span class="token class-name">pthread_attr_t</span><span class="token operator">*</span><span class="token punctuation">,</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token punctuation">)</span>    <span class="token comment">//pthread_t不透明，程序员不可操作</span>  <span class="token comment">//调用</span>errcode<span class="token operator">=</span><span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>thread_id<span class="token punctuation">,</span><span class="token operator">&amp;</span>thread_attribute<span class="token punctuation">,</span><span class="token operator">&amp;</span>thread_fun<span class="token punctuation">,</span><span class="token operator">&amp;</span>fun_arg<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>thread_id<ul><li>指针：线程ID或句柄（用于停止线程）</li></ul></li><li>thread_attribute:<ul><li>各种属性，通常用空指针<strong>NULL</strong>表示标准默认属性值</li></ul></li><li>thread_fun<ul><li>新线程要运行的函数（参数和返回值类型都是void*）</li></ul></li><li>fun_arg<ul><li>传递给要运行的函数thread_fun的参数</li></ul></li><li>errorocode<ul><li>若创建失败，返回非零值</li></ul></li></ul><p>![image-20221031151330065](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20221031151330065.png)</p><h4 id="Pthread-“hello-world”程序"><a href="#Pthread-“hello-world”程序" class="headerlink" title="Pthread “hello world”程序"></a>Pthread “hello world”程序</h4><p>![image-20221031152746925](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20221031152746925.png)</p><pre class="line-numbers language-c#" data-language="c#"><code class="language-c#">#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;pthread.h&gt; int thread_count;void* Hello(void* rank)int main(int argc,char* argv[]){  long thread;  pthread_t* thread_handles;    thread_count=strto(argv[1],NULL,10);    thread_handles=(pthread_t*)malloc(thread_count*sizeof(pthread_t*));    for(thread=0;thread&lt;thread_count;thread++) pthread_create(&amp;thread_handles[thread],NULL,Hello,(void*)thread);    printf("Hello from the main thread\n");    for(thread=0;thread&lt;thread_count;thread++);  pthread_join(thread_handles[thread],NULL);    free(thread_handles);  return 0;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Pthread其他基础API"><a href="#Pthread其他基础API" class="headerlink" title="Pthread其他基础API"></a>Pthread其他基础API</h4><ul><li><p>取消、结束线程</p><ul><li><p>void pthread_exit(void *value_ptr)</p><ul><li>显式取消线程</li><li>通过value_ptr返回结果给调用者</li></ul></li><li><p>int pthread_cnacel(pthread_t thread)</p><ul><li><p>取消线程thread执行</p></li><li><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221107141515838.png" alt="image-20221107141515838"></p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221107141538026.png" alt="image-20221107141538026"></p></li></ul></li></ul></li></ul><h3 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h3><ul><li><p>例子 估算pai</p><ul><li><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221107142015679.png" alt="image-20221107142015679"></p></li><li><p>多线程版本</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221107142246876.png" alt="image-20221107142246876"></p><p>问题：每个线程都要把数加到<strong>sum</strong>上面，会存在竞争，结果是错误的</p></li></ul></li></ul><h4 id="概念回顾"><a href="#概念回顾" class="headerlink" title="概念回顾"></a>概念回顾</h4><ul><li><p>原子性</p><ul><li>一组操作要么全部执行要么全不执行，则称其是原子性的</li></ul></li><li><p>临界区</p><ul><li>共享资源的代码段，一次只能允许一个线程执行该代码</li></ul></li><li><p>竞争条件</p><ul><li>多个线程/进程尝试更新同一个共享资源时，结果可能是无法预测的，则存在竞争</li><li>如果存在竞争则创建临界区</li></ul></li><li><p>数据依赖</p><ul><li>两个<strong>内存</strong>的序。为了保证结果正确性，必须保持这个序</li></ul></li><li><p>同步</p><ul><li>时间上强制使各执行进程/线程在某一点必须互相等待，确保各进程/线程的正常顺序和对共享可写数据的正确访问。</li></ul></li></ul><h4 id="忙等待"><a href="#忙等待" class="headerlink" title="忙等待"></a>忙等待</h4><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221107143703393.png" alt="image-20221107143703393"></p><ul><li>临界区的这段代码保证了各个进程是按照序号进行相加</li><li>这个等待消耗cpu资源</li></ul><h4 id="显式同步：互斥量（锁）"><a href="#显式同步：互斥量（锁）" class="headerlink" title="显式同步：互斥量（锁）"></a>显式同步：互斥量（锁）</h4><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221107144116394.png" alt="image-20221107144116394"></p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221107144218073.png" alt="image-20221107144218073"></p><ul><li>这个版本线程不一定按照从小到大的顺序来，使用互斥量的效率更高，先执行完的不需要等待<ul><li>操作系统选择顺序，谁先执行完谁先来</li><li>被锁上的处于阻塞态，不占用cpu资源</li></ul></li><li>锁只能保证一段时间内只有线程运行</li></ul><h5 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h5><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221107150037798.png" alt="image-20221107150037798"></p><h4 id="使用信号量同步"><a href="#使用信号量同步" class="headerlink" title="使用信号量同步"></a>使用信号量同步</h4><h4 id="使用barrier同步"><a href="#使用barrier同步" class="headerlink" title="使用barrier同步"></a>使用barrier同步</h4><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221107151930138.png" alt="image-20221107151930138"></p>]]></content>
      
      
      <categories>
          
          <category> 并行程序设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并行程序设计 </tag>
            
            <tag> S5课上 </tag>
            
            <tag> 知识总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并行程序设计-第三讲.SIMD编程</title>
      <link href="/2022/09/25/di-san-jiang.simd-bian-cheng/"/>
      <url>/2022/09/25/di-san-jiang.simd-bian-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="SIMD编程"><a href="#SIMD编程" class="headerlink" title="SIMD编程"></a>SIMD编程</h1><h2 id="SIMD概念"><a href="#SIMD概念" class="headerlink" title="SIMD概念"></a>SIMD概念</h2><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241415541.png" alt="image-20221024141506247" style="zoom:33%;"><p>SPMD单个程序在不同数据流上执行</p><p>本讲主要介绍单核向量编程</p><h3 id="SIMD编程概述"><a href="#SIMD编程概述" class="headerlink" title="SIMD编程概述"></a>SIMD编程概述</h3><ul><li><p>向量计算机</p></li><li><p>早期的SIMD超级计算机：银河</p></li><li><p>当前的SIMD架构</p><ul><li>多媒体扩展：SSE、AVX</li><li>图形和游戏处理器：CUDA</li><li>协处理器：Xeon Phi</li></ul></li><li><p>没有占压倒优势的SIMD编程模型</p><ul><li>向量计算机都是科学家用来编程 </li><li>多媒体扩展指令集多是系统程序员在用</li><li>GPU多是游戏开发者、大数据分析人员使用</li></ul></li><li><p>标量和SIMD（多媒体扩展架构）差别</p><ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241428129.png" alt="image-20221024142859008" style="zoom:25%;"></li></ul></li><li><p>多媒体扩展架构的核心</p><ul><li><p>SIMD并行</p></li><li><p>可变大小的数据域</p></li><li><p>向量长度=寄存器宽度➗类型大小</p></li><li><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241436940.png" alt="image-20221024143637806"></p><p>这里有128位寄存器，存储数据的大小由数据类型决定，比如如果存储长整型（32字节）的话，只能支持4个数同时计算</p></li></ul></li><li><p>适合应用SIMD的特点</p><ul><li><strong>规律</strong>的数据访问模式<ul><li>数据项在内存中<strong>连续存储</strong></li></ul></li><li><strong>短数据</strong>类型：8、16、32位</li><li><strong>流式</strong>数据处理，一系列处理阶段<ul><li>时间局域性，<strong>数据流重用</strong></li></ul></li><li>很多情况下可用来提升计算效率<ul><li>很多常量</li><li>循环迭代短</li></ul></li></ul></li><li><p>为什么采用SIMD</p><ul><li>更大的并发度</li><li>设计简单、重复功能单元即可</li><li>更小的芯片设计</li><li>缺点：代码很底层繁琐</li></ul></li><li><p>多媒体扩展编程</p><ul><li><p>语言/指令集扩展</p><ul><li><p>程序接口类似函数调用</p></li><li><p>C/C++：内置函数、 intrinsics</p></li><li><p>大多数编译器支持多媒体扩展</p><p>➢gcc：-march=corei7, -faltivec </p><p>SSE2: dst= _mm_add_ps(src1, src2); </p><p>AltiVec: dst= vec_add(src1, src2); </p><p>Neon： dst = vaddq_f32(src1, src2) </p><p>➢无统一标准</p></li><li><p>很多编译器支持自动编译</p></li></ul></li></ul></li></ul><h2 id="SIMD并行（不是很重要，了解即可）"><a href="#SIMD并行（不是很重要，了解即可）" class="headerlink" title="SIMD并行（不是很重要，了解即可）"></a>SIMD并行（不是很重要，了解即可）</h2><h3 id="几个例子"><a href="#几个例子" class="headerlink" title="几个例子"></a>几个例子</h3><ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241454764.png" alt="image-20221024145431584" style="zoom:25%;"><p>把重复的算术运算变成向量运算</p></li><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241502598.png" alt="image-20221024150231461" style="zoom:25%;"><p>存取数据的次数降低，效率提升</p></li><li><p>可向量化的循环</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241503733.png" alt="image-20221024150338675" style="zoom:33%;"><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241504259.png" alt="image-20221024150422081" style="zoom:33%;"></li><li><p>可部分向量化的循环（有数据依赖）</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241506128.png" alt="image-20221024150628063" style="zoom:25%;"><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241506150.png" alt="image-20221024150649061" style="zoom:25%;"></li></ul><h3 id="SIMD编程的额外开销"><a href="#SIMD编程的额外开销" class="headerlink" title="SIMD编程的额外开销"></a>SIMD编程的额外开销</h3><h4 id="打包-x2F-解包数据的开销：重排数据使之连续"><a href="#打包-x2F-解包数据的开销：重排数据使之连续" class="headerlink" title="打包/解包数据的开销：重排数据使之连续"></a>打包/解包数据的开销：重排数据使之连续</h4><ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241514360.png" alt="image-20221024151459299" style="zoom:33%;"></li><li>打包源运算对象——拷贝到连续内存区域<ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241515632.png" alt="image-20221024151536575" style="zoom:33%;"></li></ul></li><li>解包目的运算对象——拷贝回内存<ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241516236.png" alt="image-20221024151628154"></li></ul></li></ul><h4 id="对齐：调整数据访问，使之对齐"><a href="#对齐：调整数据访问，使之对齐" class="headerlink" title="对齐：调整数据访问，使之对齐"></a>对齐：调整数据访问，使之对齐</h4><ul><li><p>对齐的内存访问 </p><ul><li>地址总是向量长度的倍数（例如16字节）</li><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241523674.png" alt="image-20221024152331590" style="zoom: 33%;"></li></ul></li><li><p>未对齐的内存访问</p><ul><li><p>地址不是16字节的整数倍</p></li><li><p>静态对齐：对未对齐的读操作，做两次相邻的对齐读操作，然后进行合并</p></li><li><p>有未对齐相应操作函数，仍会产生<strong>多次内存操作</strong></p></li><li><p>底层硬件的操作：<img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241525183.png" alt="image-20221024152556088"></p></li><li><p><strong>静态</strong>调整循环</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241532617.png" alt="image-20221024153232518" style="zoom:25%;"></li><li><p>动态对齐：</p><p>不知道从几开始，虽然有额外的对齐开销，但是结果一定是正确的</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210241533834.png" alt="image-20221024153357734"></p></li></ul></li><li><p>小结</p><ul><li><p>最坏情况需要计算地址，动态对齐</p></li><li><p>编译器/程序员可分析确认对齐</p><ul><li>一般而言数据是从<strong>起始地址处对齐</strong>的</li><li>如果在一个循环中顺序访问数据，<strong>起始位置固定</strong>，则对齐特性是不变的</li></ul></li><li><p>可调整算法，先串行处理到对齐边界， 然后进行SIMD计算 </p></li><li><p>有时对齐开销会完全抵消SIMD的并行收 益</p></li></ul></li></ul><h4 id="控制流可能要求执行所有路径"><a href="#控制流可能要求执行所有路径" class="headerlink" title="控制流可能要求执行所有路径"></a>控制流可能要求执行所有路径</h4><ul><li><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210251621039.png" alt="image-20221025162120878"></p></li><li><p>底层实现</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210251622289.png" alt="image-20221025162249109" style="zoom:33%;"></li><li><p>能否改进</p><ul><li>假定所有控制流路径执行频率都不同</li><li>应该针对频率最高的路径优化代码</li><li>其他路经按默认方式执行</li><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210251624368.png" alt="image-20221025162455276" style="zoom:33%;"></li></ul></li><li><p>控制流开销小结</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210251626006.png" alt="image-20221025162609859" style="zoom: 25%;"></li></ul><h3 id="SIMD编程复杂性"><a href="#SIMD编程复杂性" class="headerlink" title="SIMD编程复杂性"></a>SIMD编程复杂性</h3><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210251627096.png" alt="image-20221025162755998" style="zoom:25%;"><h2 id="SSE-x2F-AVX编程"><a href="#SSE-x2F-AVX编程" class="headerlink" title="SSE/AVX编程"></a>SSE/AVX编程</h2><h3 id="X86架构"><a href="#X86架构" class="headerlink" title="X86架构"></a>X86架构</h3><h4 id="发展历史"><a href="#发展历史" class="headerlink" title="发展历史"></a>发展历史</h4><ul><li><p>X86：Intel开发的一种微处理器体系结构 </p></li><li><p>出现：1978年Intel 8086 CPU中 </p><ul><li>但是一般来说<strong>X86指的是X86_32bit，32位系统</strong></li><li>64位就是指X86_64bit。简写为X64</li></ul></li><li><p>发展：</p><ul><li><p>❑ 1971-1992年数字编号：80X86系列</p></li><li><p>❑ 1993-2005年奔腾系列：Pentium</p></li><li><p>❑ 2005酷睿系列：Core</p></li></ul></li></ul><h4 id="基本框架"><a href="#基本框架" class="headerlink" title="基本框架"></a>基本框架</h4><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210251635439.png" alt="image-20221025163551311" style="zoom: 50%;"><ul><li>基本的执行模式 </li><li>数据类型 </li><li>指令集合（只讲这个） <ul><li>❑ 通用指令（传送，算术，逻辑，控制等） </li><li>❑ X87 FPU指令（传送，算术，比较，控制等 ） </li><li>❑ MMX指令（传送，转化，打包，比较等）</li><li>❑ SSE指令（增加寄存器，SIMD浮点数运算）</li><li>❑ SSE2指令（整数指令，64-bit SIMD浮点运 算）</li></ul></li><li>寄存器</li></ul><h4 id="x86架构SIMD支持"><a href="#x86架构SIMD支持" class="headerlink" title="x86架构SIMD支持"></a>x86架构SIMD支持</h4><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210251641729.png" alt="image-20221025164145509" style="zoom: 50%;"><h3 id="SSE指令集"><a href="#SSE指令集" class="headerlink" title="SSE指令集"></a>SSE指令集</h3><h4 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h4><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210251645135.png" alt="image-20221025164554037" style="zoom: 25%;"><h4 id="发展历史-1"><a href="#发展历史-1" class="headerlink" title="发展历史"></a>发展历史</h4><ul><li><p>SSE是MMX的超集</p></li><li><p>MMX</p><ul><li>❑ 1996年Intel在奔腾处理器集成MMX指令， 为应对音频、图片、视频等多媒体应用的密 集的计算需求 </li><li>❑ <strong>64-bit</strong>的MMX寄存器（8个，复用了浮点寄存器的尾部，与x87<strong>共用</strong>寄存器，缺少浮点指令） </li><li>❑ 支持在打包的字，字节，双字整数上的 SIMD操作</li></ul></li><li><p>SSE128bit寄存器与MMX寄存器的区别</p><ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210251650337.png" alt="image-20221025165011232"></li><li><strong>AVX 256位</strong></li></ul></li></ul><h3 id="AVX指令集"><a href="#AVX指令集" class="headerlink" title="AVX指令集"></a>AVX指令集</h3><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210251651766.png" alt="image-20221025165156622" style="zoom: 25%;"><h3 id="SSE编程"><a href="#SSE编程" class="headerlink" title="SSE编程"></a>SSE编程</h3><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210251653652.png" alt="image-20221025165341462" style="zoom:33%;"><h4 id="SSE指令"><a href="#SSE指令" class="headerlink" title="SSE指令"></a>SSE指令</h4><h5 id="数据移动指令"><a href="#数据移动指令" class="headerlink" title="数据移动指令"></a>数据移动指令</h5><p>将数据移入/出向量寄存器</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210251658603.png" alt="image-20221025165835481" style="zoom:33%;"><h5 id="算术指令"><a href="#算术指令" class="headerlink" title="算术指令"></a>算术指令</h5><p>多个数据（2 doubles、4 floats等） 上的算术运算</p><ul><li>❑ PD：两个双精度，PS：四个单精度，SS：标量 </li><li>❑ ADD、SUB、MUL、DIV、SQRT、MAX、MIN、RCP等 <ul><li>➢ADDPS：四个单精度加法；ADDSS：标量加法</li></ul></li></ul><h5 id="逻辑指令"><a href="#逻辑指令" class="headerlink" title="逻辑指令"></a>逻辑指令</h5><p>多个数据上的逻辑运算</p><ul><li>❑ AND、OR、XOR、ANDN等 <ul><li>➢ANDPS – 运算对象位与 </li><li>➢ANDNPS – 运算对象位与非</li></ul></li></ul><h5 id="比较指令"><a href="#比较指令" class="headerlink" title="比较指令"></a>比较指令</h5><p>多个数据上的比较运算</p><p>❑ CMPPS、CMPSS：比较运算对象，每个比较结果影 响SIMD寄存器中32位——全1或全0</p><h5 id="洗牌指令"><a href="#洗牌指令" class="headerlink" title="洗牌指令"></a>洗牌指令</h5><p>在SIMD寄存器内移动数据</p><ul><li>❑ SHUFPS：从一个运算对象洗牌数据保存到另一个运 算对象 </li><li>❑ UNPCKHPS：解包高位数据到一个SIMD寄存器 <ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210251701303.png" alt="image-20221025170117239" style="zoom:33%;"></li></ul></li><li>❑ UNPCKLPS</li></ul><h5 id="其他指令"><a href="#其他指令" class="headerlink" title="其他指令"></a>其他指令</h5><ul><li>❑ 类型转换：CVTPS2PI mm,xmm/mem64 </li><li>❑ 缓存控制<ul><li>➢MOVNTPS将浮点数据从一个SIMD寄存器保存到内存，绕 过缓存</li></ul></li><li>❑ 状态管理：LDMXCSR读取MXCSR状态寄存器</li></ul><h4 id="SSE-C-x2F-C-编程"><a href="#SSE-C-x2F-C-编程" class="headerlink" title="SSE C/C++编程"></a>SSE C/C++编程</h4><ul><li><p>SSE指令对应C/C++ intrinsic </p><ul><li>❑ intrinsic：编译器能识别的函数，直接映射 为一个或多个汇编语言指令。Intrinsic函数 本质上比调用函数更高效 </li><li>❑ Intrinsics为处理器专有扩展特性提供了一个 C/C++编程接口 </li><li>❑ 主流编译器都支持，如GCC</li></ul></li><li><p>使用SSE intrinsics所需的头文件（<strong>向前兼容</strong>） </p><ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210251706442.png" alt="image-20221025170622370" style="zoom: 50%;"></li><li>向前兼容：比如我想要使用SSE3，其中就包含了SSE和SSE2</li></ul></li><li><p>编译选项： -march=corei7</p></li><li><p>AMD CPU对MMX/SSE/SSE2支持较好，SSE4支持较差</p></li></ul><h4 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h4><h5 id="串行版本"><a href="#串行版本" class="headerlink" title="串行版本"></a>串行版本</h5><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">void</span> <span class="token function">mul</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">,</span><span class="token keyword">float</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">[</span>maxN<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token keyword">float</span> b<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">[</span>maxN<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token keyword">float</span> c<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">[</span>maxN<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>j<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>j<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>      c<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">;</span>      <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> k<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>k<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>k<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        c<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">=</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token operator">*</span>b<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="cache优化"><a href="#cache优化" class="headerlink" title="cache优化"></a>cache优化</h5><p>把ab两个矩阵都变成行主序  </p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">void</span> <span class="token function">trans_mul</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">,</span><span class="token keyword">float</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">[</span>maxN<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token keyword">float</span> b<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">[</span>maxN<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token keyword">float</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">[</span>maxN<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>  <span class="token comment">//转置</span>  <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>j<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>j<span class="token operator">++</span><span class="token punctuation">)</span>      <span class="token function">swap</span><span class="token punctuation">(</span>b<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">,</span>b<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>      <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>j<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>j<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>      c<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">;</span>      <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> k<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>k<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>k<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        c<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">=</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token operator">*</span>b<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token comment">//⚠️是b[j][k],不是b[k][j]</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span>    <span class="token comment">//转置</span>  <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>j<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>j<span class="token operator">++</span><span class="token punctuation">)</span>      <span class="token function">swap</span><span class="token punctuation">(</span>b<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">,</span>b<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="SSE版本🌟"><a href="#SSE版本🌟" class="headerlink" title="SSE版本🌟"></a>SSE版本🌟</h5><p>不懂</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">void</span> <span class="token function">sse_mul</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">,</span><span class="token keyword">float</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">[</span>maxN<span class="token punctuation">]</span><span class="token punctuation">,</span>b<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">[</span>maxN<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token keyword">float</span> c<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">[</span>maxN<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>  __m128t1<span class="token punctuation">,</span>t2<span class="token punctuation">,</span>sum<span class="token punctuation">;</span>  <span class="token comment">//转置</span>  <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>j<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>j<span class="token operator">++</span><span class="token punctuation">)</span>      <span class="token function">swap</span><span class="token punctuation">(</span>b<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">,</span>b<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>j<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>j<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>      c<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>      sum<span class="token operator">=</span><span class="token function">_mm_setzero_ps</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> k<span class="token operator">=</span>n<span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">;</span>k<span class="token operator">&gt;=</span><span class="token number">0</span><span class="token punctuation">;</span>k<span class="token operator">-=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        t1<span class="token operator">=</span><span class="token function">_mm_loadu_ps</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">+</span>k<span class="token punctuation">)</span><span class="token punctuation">;</span>        t2<span class="token operator">=</span><span class="token function">_mm_loadu_ps</span><span class="token punctuation">(</span>b<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">+</span>k<span class="token punctuation">)</span><span class="token punctuation">;</span>        t1<span class="token operator">=</span><span class="token function">_mm_mul_ps</span><span class="token punctuation">(</span>t1<span class="token punctuation">,</span>t2<span class="token punctuation">)</span><span class="token punctuation">;</span>        sum<span class="token operator">=</span><span class="token function">_mm_add_ps</span><span class="token punctuation">(</span>sum<span class="token punctuation">,</span>t1<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span>      sum<span class="token operator">=</span><span class="token function">_mm_hadd_ps</span><span class="token punctuation">(</span>sum<span class="token punctuation">,</span>sum<span class="token punctuation">)</span><span class="token punctuation">;</span>      sum<span class="token operator">=</span><span class="token function">_mm_hadd_ps</span><span class="token punctuation">(</span>sum<span class="token punctuation">,</span>sum<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token function">_mm_store_ss</span><span class="token punctuation">(</span>c<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">+</span>j<span class="token punctuation">,</span>sum<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token comment">//如果还有不能被4整除的部分</span>      <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> k<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token operator">%</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span>k<span class="token operator">&gt;=</span><span class="token number">0</span><span class="token punctuation">;</span>k<span class="token operator">--</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        c<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">+=</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token operator">*</span>b<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span>        <span class="token comment">//转置</span>  <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>j<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>j<span class="token operator">++</span><span class="token punctuation">)</span>      <span class="token function">swap</span><span class="token punctuation">(</span>b<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">,</span>b<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="分片策略"><a href="#分片策略" class="headerlink" title="分片策略"></a>分片策略</h5><p>啥玩意啊</p><p>看不懂</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221101214350768.png" alt="image-20221101214350768"></p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221101214331724.png" alt="image-20221101214331724"></p>]]></content>
      
      
      <categories>
          
          <category> 并行程序设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并行程序设计 </tag>
            
            <tag> S5课上 </tag>
            
            <tag> 知识总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python-面向对象编程</title>
      <link href="/2022/09/17/mian-xiang-dui-xiang-bian-cheng/"/>
      <url>/2022/09/17/mian-xiang-dui-xiang-bian-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="面向对象编程"><a href="#面向对象编程" class="headerlink" title="面向对象编程"></a>面向对象编程</h1><h2 id="私有方法"><a href="#私有方法" class="headerlink" title="私有方法"></a>私有方法</h2><ul><li>__private_method:<ul><li>两个下划线开头</li><li>私有方法，只能在类的内部调用，不能在类的外部调用</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Site</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>name<span class="token punctuation">,</span>url<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">,</span>name<span class="token operator">=</span>name<span class="token comment">#public</span>    self<span class="token punctuation">.</span>__url<span class="token operator">=</span>url<span class="token comment">#private</span>      <span class="token keyword">def</span> <span class="token function">who</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="类之间的关系"><a href="#类之间的关系" class="headerlink" title="类之间的关系"></a>类之间的关系</h2><ul><li>is-a<ul><li>继承</li></ul></li><li>Has-a<ul><li>组合或关联</li><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221109160623400.png" alt="image-20221109160623400"></li><li>例：池塘里里面的各种生物</li></ul></li><li>Use-a<ul><li>依赖</li><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221109160605227.png" alt="image-20221109160605227"></li></ul></li></ul><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102162738720.png" alt="image-20221102162738720"></p><h3 id="类的继承"><a href="#类的继承" class="headerlink" title="类的继承"></a>类的继承</h3><ul><li>基类（也叫父类）<ul><li>被继承的类</li></ul></li><li>派生类（也叫子类）<ul><li>继承的类</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">People</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>name<span class="token punctuation">,</span>age<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>name<span class="token operator">=</span>name    self<span class="token punctuation">.</span>age<span class="token operator">=</span>age      <span class="token keyword">def</span> <span class="token function">speak</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token operator">//</span>这里的self不一定是People的    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>name<span class="token punctuation">}</span></span><span class="token string">说：我</span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>age<span class="token punctuation">}</span></span><span class="token string">岁。"</span></span><span class="token punctuation">)</span>        <span class="token keyword">class</span> <span class="token class-name">Student</span><span class="token punctuation">(</span>People<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__int__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>name<span class="token punctuation">,</span>age<span class="token punctuation">,</span>weight<span class="token punctuation">,</span>grade<span class="token punctuation">)</span><span class="token punctuation">:</span>    People<span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>self<span class="token punctuation">,</span>name<span class="token punctuation">,</span>age<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>weight<span class="token operator">=</span>weight    self<span class="token punctuation">.</span>grade<span class="token operator">=</span>grade    xm<span class="token operator">=</span>Student<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"小明"</span><span class="token punctuation">,</span>age<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>weight<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>grade<span class="token operator">=</span><span class="token string">"三年级"</span><span class="token punctuation">)</span><span class="token operator">//</span>也可以直接输入值xm<span class="token punctuation">.</span>speak<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">//</span>谁调了他，就是谁传了进去<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="方法的重写"><a href="#方法的重写" class="headerlink" title="方法的重写"></a>方法的重写</h4><p>子类会覆盖父类</p><h4 id="多继承"><a href="#多继承" class="headerlink" title="多继承"></a>多继承</h4><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102163858056.png" alt="image-20221102163858056"></p><ul><li>A(B,E)顺序不能变❕<ul><li>表示A先继承B</li></ul></li><li>G自动继承object类</li></ul><h5 id="MRO算法"><a href="#MRO算法" class="headerlink" title="MRO算法"></a>MRO算法</h5><ul><li>使用_ <em>mro</em> _来查询print(D.__mro _)</li><li>使用merge算法进行推导（不是重点，可以了解）</li></ul><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102164627442.png" alt="image-20221102164627442"></p><ul><li>利用剪枝<ul><li>左优先‼️</li></ul></li><li>找如度为0的把它去掉</li></ul><h4 id="super（）"><a href="#super（）" class="headerlink" title="super（）"></a>super（）</h4><ul><li>super（）语法：</li><li></li></ul><p>🌟考试会考</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102170836759.png" alt="image-20221102170836759"></p><ul><li><p>例<img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102171352784.png" alt="image-20221102171352784"></p><ul><li><p>mro顺序</p></li><li><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102171435329.png" alt="image-20221102171435329"></p></li></ul></li><li><p>例</p><ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102172330714.png" alt="image-20221102172330714"></li><li>看的是mro的顺序</li><li>c➡️a➡️b➡️base</li></ul></li><li><p>例</p><ul><li>理解self是谁</li><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102172943838.png" alt="image-20221102172943838"></li></ul></li><li><p>super0和类分别调用父类实例方法，区别在于super0后跟的方法不需要传self，父类调用实例方法，第一个参数需要传self</p></li></ul><h2 id="类的多态"><a href="#类的多态" class="headerlink" title="类的多态"></a>类的多态</h2><p>只要有方法就可以调用</p><p>python不用定义接口，其他语言需要定义接口</p><ul><li>不同的对象调用同一个接口，从而表现出不同的状态，称为多态</li><li>多态发生的条件：<ul><li>继承：发生在父类子类之间</li><li>重写：子类重写父类方法</li></ul></li><li>增加一个功能<ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221109164508768.png" alt="image-20221109164508768"></li></ul></li></ul><h3 id="多态的约束性方法"><a href="#多态的约束性方法" class="headerlink" title="多态的约束性方法"></a>多态的约束性方法</h3><ul><li><p>通过直接抛异常的方式</p><ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221109170156871.png" alt="image-20221109170156871"></li></ul></li><li><p>通过抽象基类（基类就是父类）和抽象方法来实现</p><ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221109170736639.png" alt="image-20221109170736639"></li></ul></li></ul><h2 id="类的Mixin设计模式"><a href="#类的Mixin设计模式" class="headerlink" title="类的Mixin设计模式"></a>类的Mixin设计模式</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Vehicle</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">fly</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"I am flying"</span><span class="token punctuation">,</span>self<span class="token punctuation">)</span>    <span class="token keyword">class</span> <span class="token class-name">CivilAircraft</span><span class="token punctuation">(</span>Vehicle<span class="token punctuation">)</span>  <span class="token punctuation">:</span>  <span class="token keyword">pass</span><span class="token keyword">class</span> <span class="token class-name">Helicopter</span><span class="token punctuation">(</span>Vehicle<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">pass</span><span class="token keyword">class</span> <span class="token class-name">Car</span><span class="token punctuation">(</span>Vehicle<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>car类继承vehicle类，但是car没有飞行功能</li><li>民航飞机与直升飞机重写fly代码冗余</li></ul><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221109162525708.png" alt="image-20221109162525708"></p><ul><li>一种设计模式</li><li>单独定义一个Mixin类<ul><li>比喻：飞行只是飞机的一个增强属性</li></ul></li><li>python中没有接口一说</li></ul><h3 id="Mixin类实现多继承需要遵循的原则"><a href="#Mixin类实现多继承需要遵循的原则" class="headerlink" title="Mixin类实现多继承需要遵循的原则"></a>Mixin类实现多继承需要遵循的原则</h3><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221109163244822.png" alt="image-20221109163244822"></p><h3 id="不使用Mixin的弊端"><a href="#不使用Mixin的弊端" class="headerlink" title="不使用Mixin的弊端"></a>不使用Mixin的弊端</h3><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221109163332608.png" alt="image-20221109163332608"></p><h2 id="类中的绑定方法（有同学问了讲的）"><a href="#类中的绑定方法（有同学问了讲的）" class="headerlink" title="类中的绑定方法（有同学问了讲的）"></a>类中的绑定方法（有同学问了讲的）</h2><h3 id="实例的绑定方法"><a href="#实例的绑定方法" class="headerlink" title="实例的绑定方法"></a>实例的绑定方法</h3><h3 id="类的绑定方法"><a href="#类的绑定方法" class="headerlink" title="类的绑定方法"></a>类的绑定方法</h3><p>通过classmethod装饰器，将绑定给实例的方法绑定到了类</p><p>谁调用了他就把谁传给cls</p><p>实例调用会把生成实例的类传给cls</p><h3 id="类的非绑定方法"><a href="#类的非绑定方法" class="headerlink" title="类的非绑定方法"></a>类的非绑定方法</h3><p>通过staticmethod装饰器，可以解除绑定关系，既不属于实例也不属于类</p><p>和普通函数相同，不用考虑自动传参</p><h2 id="描述器-描述符-协议"><a href="#描述器-描述符-协议" class="headerlink" title="描述器(描述符)协议"></a>描述器(描述符)协议</h2><ul><li><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221109171624111.png" alt="image-20221109171624111"></p></li><li><p>父类为object，object里面已经实现了。因此可以直接调用a</p></li><li><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221116164118563.png" alt="image-20221116164118563"></p><ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221116164222818.png" alt="image-20221116164222818"><ul><li>动态生成</li></ul></li></ul></li><li><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221109172711106.png" alt="image-20221109172711106"></p><ul><li>M（）成为描述器实例</li><li>AA中把实例m当作自己的属性</li></ul></li></ul><h3 id="数据描述器和非数据描述器"><a href="#数据描述器和非数据描述器" class="headerlink" title="数据描述器和非数据描述器"></a>数据描述器和非数据描述器</h3><p>数据描述器也叫资料描述器</p><p>🌟getattr函数可以做反射</p><p>用字符串映射有没有这个方法</p><p>掌握：</p><p>什么叫描述器</p><p>数据描述器和非数据描述器</p><p>getattr</p><h3 id="描述器用途"><a href="#描述器用途" class="headerlink" title="描述器用途"></a>描述器用途</h3><ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221116164924867.png" alt="image-20221116164924867"></li><li>vars和_ _dict _ _协议相同</li></ul><h3 id="描述器访问顺序（🌟重要）"><a href="#描述器访问顺序（🌟重要）" class="headerlink" title="描述器访问顺序（🌟重要）"></a>描述器访问顺序（🌟重要）</h3><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221116172952732.png" alt="image-20221116172952732"></p><ul><li>只是定义了A类的时候就输出了前两行</li></ul><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221116174035448.png" alt="image-20221116174035448"></p><h2 id="判断类实例的函数"><a href="#判断类实例的函数" class="headerlink" title="判断类实例的函数"></a>判断类实例的函数</h2><ul><li><p>type和isinistance函数的区别</p><ul><li><p>isinstance（obj，cls）：判断obj是否是cls类·或者<strong>他的子类</strong>的实例</p></li><li><p>type（obj）：获取实例obj的类型，<strong>不考虑继承关系</strong></p><ul><li><p>与__class _ _ 作用相同</p></li><li><p>二者等价</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221116161439404.png" alt="image-20221116161439404"></p></li></ul></li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">A</span> <span class="token punctuation">:</span>  <span class="token keyword">pass</span><span class="token builtin">type</span><span class="token punctuation">(</span>A（）<span class="token punctuation">)</span><span class="token comment">#结果是A</span><span class="token builtin">type</span>（A）<span class="token comment">#结果是&lt;class 'type'&gt;，type是生成类的类</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>元类</p><ul><li><p>创建类的类</p><ul><li><p><strong>默认的元类：type</strong></p></li><li><p>要创建一个class对象，type（）函数依次传入3个参数：</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221116161829260.png" alt="image-20221116161829260"></p></li></ul></li><li><p>通过继承type创建元类</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221116163027470.png" alt="image-20221116163027470"></p><ul><li>什么都不用做都可以输出前两行，只是定义就可以（🌟，考试没准就考）<ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221116162459971.png" alt="image-20221116162459971"></li><li>元类是生成类，</li></ul></li></ul></li></ul></li></ul><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221116163626790.png" alt="image-20221116163626790"></p><ul><li>object可以指这三种类型，具体情况具体分析</li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> S5课上 </tag>
            
            <tag> 知识总结 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并行程序设计-实验2.SSE编程练习</title>
      <link href="/2022/09/13/sse-bian-cheng-lian-xi/"/>
      <url>/2022/09/13/sse-bian-cheng-lian-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="SSE编程练习"><a href="#SSE编程练习" class="headerlink" title="SSE编程练习"></a>SSE编程练习</h1><h2 id="矩阵乘法的优化"><a href="#矩阵乘法的优化" class="headerlink" title="矩阵乘法的优化"></a>矩阵乘法的优化</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>一个n行n列的矩阵a与一个n行n列的矩阵b的乘积，是一个n行n列的矩阵c。本实验分别使用了串行算法、Cache 优化、SSE 编程和分片策略四种算法实现了矩阵乘法，并就各算法的执行性能进行比较与分析。</p><h3 id="算法设计实现与复杂性分析"><a href="#算法设计实现与复杂性分析" class="headerlink" title="算法设计实现与复杂性分析"></a>算法设计实现与复杂性分析</h3><h4 id="串行算法"><a href="#串行算法" class="headerlink" title="串行算法"></a>串行算法</h4><p>​依据矩阵乘法规则，矩阵a的每一行与对应b的每一列分别相乘后求和，设计三层循环嵌套，按矩阵a的行主序处理。</p><p>​具体代码如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void mul(float a[maxN][maxN], float b[maxN][maxN]){    for (int i = 0; i &lt; n; ++i) {        for (int j = 0; j &lt; n; ++j) {            c[i][j] = 0.0;            for (int k = 0; k &lt; n; ++k) {                c[i][j] += a[i][k] * b[k][j];            }        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​通过代码可以看出，串行算法的设计中有3个for循环，一个for循环是O(n)，所以串行算法的复杂度为O(n *n *n)=O(n^3)</p><h4 id="cache优化"><a href="#cache优化" class="headerlink" title="cache优化"></a>cache优化</h4><p>​从上面串行算法的代码中可以看出，b [k] [j]在读取内存中的数据时是不连续的。在最底层的循环中，随着k不断加一，b[k] [j]不断地在内存中跳跃。这会引起cache命中率低，循环程序不断的把内存转移到缓存中，引起效率降低。</p><p>​cache优化的思路便是通过将b矩阵转置，以行主序的方式来访问b矩阵，这样保证了b矩阵空间访问的局部性，效率得到提高。</p><p>​具体代码如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void trans_mul(float a[maxN][maxN], float b[maxN][maxN]){    //转置    for (int i = 0; i &lt; n; ++i)        for (int j = 0; j &lt; i; ++j){            swap(b[i][j], b[j][i]);        }    for (int i = 0; i &lt; n; ++i) {        for (int j = 0; j &lt; n; ++j) {            c[i][j] = 0.0;            for (int k = 0; k &lt; n; ++k) {                c[i][j] += a[i][k] * b[j][k];            }        }    }    //转置    for (int i = 0; i &lt; n; ++i)        for (int j = 0; j &lt; i; ++j) {            swap(b[i][j], b[j][i]);        }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​通过代码可以看出，此算法的设计中有3个for循环，一个for循环是O(n)，所以cache优化算法的复杂度为O(n *n *n)=O(n^3)</p><h4 id="SSE版本"><a href="#SSE版本" class="headerlink" title="SSE版本"></a>SSE版本</h4><p>​矩阵乘法向量化，在cache优化的基础上，利用intrisic 中的函数，以4为步长，每次处理四个对应数据，求和后存入sum，两次相邻求和，最终sum为4维向量，均为同一个数。</p><p>​具体代码如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void sse_mul(float a[maxN][maxN], float b[maxN][maxN]){    __m128 t1, t2, sum;    for (int i = 0; i &lt; n; ++i)        for (int j = 0; j &lt; i; ++j) {            swap(b[i][j], b[j][i]);        }    for (int i = 0; i &lt; n; ++i){        for (int j = 0; j &lt; n; ++j){            c[i][j] = 0.0;            sum = _mm_setzero_ps();            for (int k = n - 4; k &gt;= 0; k -= 4){                t1 = _mm_loadu_ps(a[i] + k);                t2 = _mm_loadu_ps(b[j] + k);                t1 = _mm_mul_ps(t1, t2);                sum = _mm_add_ps(sum, t1);            }            sum = _mm_hadd_ps(sum, sum);            sum = _mm_hadd_ps(sum, sum);            _mm_store_ss(c[i] + j, sum);            //不能被4整除部分的处理            for (int k = (n % 4) - 1; k &gt;= 0; --k){                c[i][j] += a[i][k] * b[j][k];            }        }    }    for (int i = 0; i &lt; n; ++i)        for (int j = 0; j &lt; i; ++j) {            swap(b[i][j], b[j][i]);        }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​此算法一次处理四个数据，算法时间复杂度为O((n^3)/4)</p><h4 id="分片策略"><a href="#分片策略" class="headerlink" title="分片策略"></a>分片策略</h4><p>​将矩阵a和b分割成子矩阵，子矩阵分别计算，合并后结果与原结果相同。根据这一原理，将矩阵划分为T个矩阵块，n维矩阵元素进入Cache中的次数为n/T，即在SSE版本未分片的基础上减少循环次数.</p><p>​具体代码如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void sse_tile(float a[maxN][maxN], float b[maxN][maxN]){    __m128 t1, t2, sum;    float t;    for (int i = 0; i &lt; n; ++i)        for (int j = 0; j &lt; i; ++j) {            swap(b[i][j], b[j][i]);        }    for (int r = 0; r &lt; n / T; ++r)        for (int q = 0; q &lt; n / T; ++q){            for (int i = 0; i &lt; T; ++i)                for (int j = 0; j &lt; T; ++j){                    c[r * T + i][q * T + j] = 0.0;                }            for (int p = 0; p &lt; n / T; ++p){                for (int i = 0; i &lt; T; ++i)                    for (int j = 0; j &lt; T; ++j){                        sum = _mm_setzero_ps();                        for (int k = 0; k &lt; T; k += 4){                            t1 = _mm_loadu_ps(a[r * T + i] + p * T + k);                            t2 = _mm_loadu_ps(b[q * T + j] + p * T + k);                            t1 = _mm_mul_ps(t1, t2);                            sum = _mm_add_ps(sum, t1);                        }                        sum = _mm_hadd_ps(sum, sum);                        sum = _mm_hadd_ps(sum, sum);                        _mm_store_ss(&amp;t, sum);                        c[r * T + i][q * T + j] += t;                    }            }        }    for (int i = 0; i &lt; n; ++i)        for (int j = 0; j &lt; i; ++j) {            swap(b[i][j], b[j][i]);        }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​算法的时间复杂度仍然为O(n^3)，但因为充分利用了高速缓存，故计算性能也会有较显著的提升。</p><h3 id="细节设计"><a href="#细节设计" class="headerlink" title="细节设计"></a>细节设计</h3><h4 id="计时方法"><a href="#计时方法" class="headerlink" title="计时方法"></a>计时方法</h4><p>​计时方面，我使用了课件中给出的Windows下精准计时方法，即QueryPerformance系列API。</p><p>​以串行算法的计时为例，具体代码如下：</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/img_v2_2dc2a1b8-4191-4cbe-8c15-681dc3bea3ag.jpg" alt="img_v2_2dc2a1b8-4191-4cbe-8c15-681dc3bea3ag"></p><h4 id="计时精度"><a href="#计时精度" class="headerlink" title="计时精度"></a>计时精度</h4><p>​本实验随机生成了30个矩阵，将计算过程重复了30次，足以匹配计算计时精度，再计算平均时间。</p><p>​以串行算法为例，具体代码如下：</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/img_v2_2dc2a1b8-4191-4cbe-8c15-681dc3bea3ag.jpg" alt="img_v2_2dc2a1b8-4191-4cbe-8c15-681dc3bea3ag"></p><h4 id="实验数据设计"><a href="#实验数据设计" class="headerlink" title="实验数据设计"></a>实验数据设计</h4><p>​本实验测试了不同规模的矩阵，由小至大，分别测试了规模为0、10、20、30……100的矩阵，随机生成矩阵元素值。</p><p>​随机生成矩阵的过程为：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">// 随机生成矩阵void init(Matrix* dataset) {    srand(static_cast &lt;unsigned&gt; (time(0)));    for (int i = 0; i &lt; 30; i++) {        for (int j = 0; j &lt; n; j++) {            for (int k = 0; k &lt; n; k++) {                dataset[i].a[j][k] = static_cast &lt;float&gt; (rand()) / (static_cast &lt;float&gt;(RAND_MAX / 1000));                dataset[i].b[j][k] = static_cast &lt;float&gt; (rand()) / (static_cast &lt;float&gt;(RAND_MAX / 1000));            }        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h4><p>​为了降低误差，本实验重复了两次取平均值。</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/img_v2_83868f60-ccae-47cf-9adb-b636e66fbdeg.jpg" alt="img_v2_83868f60-ccae-47cf-9adb-b636e66fbdeg"></p><h3 id="实验及结果分析"><a href="#实验及结果分析" class="headerlink" title="实验及结果分析"></a>实验及结果分析</h3><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/img_v2_b199d9e4-5be3-4569-86ac-7940008ed5bg.jpg" alt="img_v2_b199d9e4-5be3-4569-86ac-7940008ed5bg"></p><p>​从图中可以看出，算法运行所需要的时间：串行算法&gt;cache优化算法&gt;sse优化&gt;分片策略。</p><p>​当矩阵规模较小时串行算法和cache优化算法的性能相差不大，但随着规模的增大，cache优化算法要优于串行算法。</p><p>​可见，与串行算法相比，cache优化算法、sse版本以及分片策略提升了算法的效率，缩短了运行时间。</p><h2 id="高斯消元法SSE并行化"><a href="#高斯消元法SSE并行化" class="headerlink" title="高斯消元法SSE并行化"></a>高斯消元法SSE并行化</h2><h3 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h3><p>​高斯消元法，是线性代数中的一个算法，可用来求解线性方程组。 高斯消元法的原理是：若用初等行变换将增广矩阵化为行阶梯矩阵 ，则AX = B与CX = D是同解方程组。所以我们可以用初等行变换把增广矩阵转换为行阶梯阵，然后回代求出方程的解。</p><p>​本实验分别使用了串行算法、SSE 编程分别实现了高斯消元法和回代求解的过程，并就各算法的执行性能进行比较与分析。</p><h3 id="算法设计实现与复杂性分析-1"><a href="#算法设计实现与复杂性分析-1" class="headerlink" title="算法设计实现与复杂性分析"></a>算法设计实现与复杂性分析</h3><h4 id="串行算法消元"><a href="#串行算法消元" class="headerlink" title="串行算法消元"></a>串行算法消元</h4><p>​按行处理，以主对角线元素为标准，主对角线右侧的值除以该行主对角线上的值，最后将对角线赋值为1。每一行处理完毕，下一行依次减去上行的值，此时值需要通过当前行的第一个不为0的元素进行还原。将对角线左侧的值均赋值为0。</p><p>​具体代码如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void serialGauss(Matrix* dataset) {    for (int count = 0; count &lt; 50; count++) {        for (int i = 0; i &lt; n; i++) {            for (int j = i + 1; j &lt; n; j++) {                float temp = dataset[count].a[j][i] / dataset[count].a[i][i];                for (int k = i; k &lt; n; k++) {                    dataset[count].a[j][k] = dataset[count].a[j][k] - temp * dataset[count].a[i][k];                }            }        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​通过代码可以看出，此算法的设计中有3个for循环，一个for循环是O(n)，所以串行算法消元的复杂度为O(n *n *n)=O(n^3)</p><h4 id="sse消元"><a href="#sse消元" class="headerlink" title="sse消元"></a>sse消元</h4><p>​以4为处理单位，仍旧按行处理，每一行的数据可一次处理4个。</p><p>​具体代码如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void sseGauss(Matrix* dataset) {    for (int count = 0; count &lt; 50; count++) {        __m128 t1, t2, t3, t4;        for (int k = 0; k &lt; n; k++) {            float tmp[4] = { dataset[count].a[k][k], dataset[count].a[k][k], dataset[count].a[k][k], dataset[count].a[k][k] };            t1 = _mm_loadu_ps(tmp);            for (int j = n - 4; j &gt;= k; j -= 4) {                t2 = _mm_loadu_ps(dataset[count].a[k] + j);                t3 = _mm_div_ps(t2, t1);                _mm_storeu_ps(dataset[count].atemp[k] + j, t3);            }            if (k % 4 != (n % 4)) {                for (int j = k; j % 4 != (n % 4); j++) {                    dataset[count].atemp[k][j] = dataset[count].a[k][j] / tmp[0];                }            }            for (int j = (n % 4) - 1; j &gt;= 0; j--) {                dataset[count].atemp[k][j] = dataset[count].a[k][j] / tmp[0];            }            for (int i = k + 1; i &lt; n; i++) {                float tmp[4] = { dataset[count].a[i][k], dataset[count].a[i][k], dataset[count].a[i][k], dataset[count].a[i][k] };                t1 = _mm_loadu_ps(tmp);                for (int j = n - 4; j &gt; k; j -= 4) {                    t2 = _mm_loadu_ps(dataset[count].a[i] + j);                    t3 = _mm_loadu_ps(dataset[count].atemp[k] + j);                    t4 = _mm_sub_ps(t2, _mm_mul_ps(t1, t3));                    _mm_storeu_ps(dataset[count].a[i] + j, t4);                }                for (int j = k + 1; j % 4 != (n % 4); j++) {                    dataset[count].a[i][j] = dataset[count].a[i][j] - dataset[count].a[i][k] * dataset[count].atemp[k][j];                }                dataset[count].a[i][k] = 0;            }        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​循环每次跳跃4个元素，算法时间复杂度为O((n^3)/4)</p><h4 id="串行实现回代"><a href="#串行实现回代" class="headerlink" title="串行实现回代"></a>串行实现回代</h4><p>​将得到的上三角矩阵，从第 n 行开始，倒序回代到前面的每一行，即可求解 Xn 到 X1 的值。</p><p>​具体代码如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">    for (int count = 0; count &lt; 50; count++) {        for (int i = n - 1; i &gt;= 0; i--) {            float sum = 0;            for (int j = n - 1; j &gt; i; j--) {                sum += dataset[count].a[i][j] * dataset[count].x[j];            }            dataset[count].x[i] = (dataset[count].b[i] - sum) / dataset[count].a[i][i];        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​算法的时间复杂度为 O(n) = n^2 </p><h4 id="sse实现回代"><a href="#sse实现回代" class="headerlink" title="sse实现回代"></a>sse实现回代</h4><p>​和串行实现回代相比，向量化计算可以同时处理四个元素。</p><p>​具体代码为：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void sseBack(Matrix* dataset) {    __m128 t1, t2, sumSSE;    float sum;    for (int count = 0; count &lt; 50; count++) {        for (int i = n - 1; i &gt;= 0; i--) {            if ((n - 1 - i) == 4)break;            float sum = 0;            for (int j = n - 1; j &gt; i; j--) {                sum += dataset[count].a[i][j] * dataset[count].x[j];            }            dataset[count].x[i] = (dataset[count].b[i] - sum) / dataset[count].a[i][i];        }        for (int i = n - 5; i &gt;= 0; i--) {            sumSSE = _mm_setzero_ps();            sum = 0;            int forSerial = (n - i - 1) % 4;            for (int j = i + forSerial + 1; j &lt;= n - 4; j += 4) {                t1 = _mm_loadu_ps(dataset[count].a[i] + j);                t2 = _mm_loadu_ps(dataset[count].x + j);                t1 = _mm_mul_ps(t1, t2);                sumSSE = _mm_add_ps(sumSSE, t1);            }            sumSSE = _mm_hadd_ps(sumSSE, sumSSE);            sumSSE = _mm_hadd_ps(sumSSE, sumSSE);            _mm_store_ss(&amp;sum, sumSSE);            for (int j = i + 1; j &lt;= i + forSerial; j++) {                sum += dataset[count].a[i][j] * dataset[count].x[j];            }            dataset[count].x[i] = (dataset[count].b[i] - sum) / dataset[count].a[i][i];        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​算法时间复杂度 O(n) = ((n^2) / 4)</p><h3 id="细节设计-1"><a href="#细节设计-1" class="headerlink" title="细节设计"></a>细节设计</h3><h4 id="计时方法-1"><a href="#计时方法-1" class="headerlink" title="计时方法"></a>计时方法</h4><p>​计时方面，我使用了课件中给出的Windows下精准计时方法，即QueryPerformance系列API。</p><p>​以串行消元算法的计时为例，具体代码如下：</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/img_v2_bfdf75f4-6f26-4787-b294-c4a1cda649ag.jpg" alt="img_v2_bfdf75f4-6f26-4787-b294-c4a1cda649ag"></p><h4 id="计时精度-1"><a href="#计时精度-1" class="headerlink" title="计时精度"></a>计时精度</h4><p>​本实验随机生成了50个矩阵，将计算过程重复了50次，足以匹配计算计时精度，再计算平均时间。</p><p>​以串行算法为例，具体代码如下：</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/img_v2_bfdf75f4-6f26-4787-b294-c4a1cda649ag.jpg" alt="img_v2_bfdf75f4-6f26-4787-b294-c4a1cda649ag"></p><h4 id="实验数据设计-1"><a href="#实验数据设计-1" class="headerlink" title="实验数据设计"></a>实验数据设计</h4><p>​本实验测试了不同规模的矩阵，由小至大，分别测试了规模为0、50、100、150……400的矩阵，随机生成矩阵元素值。</p><p>​随机生成矩阵的过程为：</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/img_v2_e3f8c442-f0a3-4071-904d-21613cffdb8g.jpg" alt="img_v2_e3f8c442-f0a3-4071-904d-21613cffdb8g"></p><h4 id="实验方法-1"><a href="#实验方法-1" class="headerlink" title="实验方法"></a>实验方法</h4><p>为了降低误差，本实验重复了两次取平均值。</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/img_v2_79adafc4-a4a9-4e25-aaec-9dcb5816256g.jpg" alt="img_v2_79adafc4-a4a9-4e25-aaec-9dcb5816256g"></p><h3 id="实验及结果分析-1"><a href="#实验及结果分析-1" class="headerlink" title="实验及结果分析"></a>实验及结果分析</h3><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/img_v2_081775d8-9d8d-4f52-a840-6b6f7b18085g.jpg" alt="img_v2_081775d8-9d8d-4f52-a840-6b6f7b18085g"></p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/img_v2_4559f7ab-1b62-4521-b9a7-a7a77f98123g.jpg" alt="img_v2_4559f7ab-1b62-4521-b9a7-a7a77f98123g"></p><p>​由图可知，SSE 算法的消元总体来说是运行效率是高于串行算法消元的;串行回代的效率也高于sse回代。</p><p>​同时，通过 Excel 中趋势线的计算，可以得出消元算法的时间复杂度符合O(n^3)，验证了前面对时间复杂度的分析。同时也可以得出回代过程的时间复杂度为 O(n^2)</p><h3 id="不同算法策略对性能的影响"><a href="#不同算法策略对性能的影响" class="headerlink" title="不同算法策略对性能的影响"></a>不同算法策略对性能的影响</h3><h4 id="相同算法对于不同问题规模的性能提升是否有影响，影响情况如何"><a href="#相同算法对于不同问题规模的性能提升是否有影响，影响情况如何" class="headerlink" title="相同算法对于不同问题规模的性能提升是否有影响，影响情况如何"></a>相同算法对于不同问题规模的性能提升是否有影响，影响情况如何</h4><p>​从2.4实验结果的图表中可以看出，在问题规模较小时，串行消元和sse消元运行时间相差不大，sse消元法对于性能的提升不明显；但是当问题的规模逐渐变大sse算法明显要优于串行算法，对性能的提升更大。</p><h4 id="回代过程可否向量化，有的话性能提升情况如何"><a href="#回代过程可否向量化，有的话性能提升情况如何" class="headerlink" title="回代过程可否向量化，有的话性能提升情况如何"></a>回代过程可否向量化，有的话性能提升情况如何</h4><p>​回代过程可以向量化，根据下图可以看出回代过程采用向量化明显提升了运行效率，和串行回代相比缩短了运行时间。随着规模的增大，性能提升更显著。</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/img_v2_4559f7ab-1b62-4521-b9a7-a7a77f98123g.jpg" alt="img_v2_4559f7ab-1b62-4521-b9a7-a7a77f98123g"></p>]]></content>
      
      
      <categories>
          
          <category> 并行程序设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并行程序设计 </tag>
            
            <tag> S5课上 </tag>
            
            <tag> 实验报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并行程序设计-第二讲.并行硬件与并行软件</title>
      <link href="/2022/09/13/di-er-jiang-bing-xing-ying-jian-he-bing-xing-ruan-jian/"/>
      <url>/2022/09/13/di-er-jiang-bing-xing-ying-jian-he-bing-xing-ruan-jian/</url>
      
        <content type="html"><![CDATA[<h1 id="第二讲-并行硬件和并行软件"><a href="#第二讲-并行硬件和并行软件" class="headerlink" title="第二讲 并行硬件和并行软件"></a>第二讲 并行硬件和并行软件</h1><h2 id="冯诺依曼结构"><a href="#冯诺依曼结构" class="headerlink" title="冯诺依曼结构"></a>冯诺依曼结构</h2><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210102147832.png" alt="image-20221010214750673" style="zoom:50%;"><p>缺点：cpu和存储器离得太远，取指令时间很长</p><h3 id="冯诺依曼模型改进"><a href="#冯诺依曼模型改进" class="headerlink" title="冯诺依曼模型改进"></a>冯诺依曼模型改进</h3><h4 id="利用cache"><a href="#利用cache" class="headerlink" title="利用cache"></a>利用cache</h4><ul><li><p>多级缓存</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210102209318.png" style="zoom:25%;"></li><li><p>cpu向cache写数据时，cache和主存中的值不一致的问题</p><ul><li>写直达：当CPU 向Cache写数据时，高速缓存行会立即写入 主存中。</li><li>写回：Cache中，数据不是立即 更新到主存中，而是将发生数据更新的高速 缓存行标记称脏(dirty)。当发生高速缓存行 替换时，标记为脏的高速缓存行被写入主存 中。</li></ul></li></ul><h4 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h4><p>内存不够用时，再磁盘中开辟一块区域作为虚拟内存</p><p>但是常用的指令还是要放到内存中</p><h4 id="指令级并行ILP"><a href="#指令级并行ILP" class="headerlink" title="指令级并行ILP"></a>指令级并行ILP</h4><p>通过让多个处理器或者功能单元同时执 行指令来提高处理器的性能。</p><ul><li><p><strong>流水线</strong>：将功能单元分阶段安排</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210102228662.png" style="zoom:25%;"></li><li><p><strong>多发射</strong>：让多条指令同时启动</p><ul><li><strong>静态</strong>多发射：功能单元在<strong>编译</strong>时调度</li><li><strong>动态</strong>多发射：功能单元再<strong>运行</strong>时间调度<ul><li>支持动态多发射的处理器叫做<strong>超标量</strong></li></ul></li><li>这两种发射方式都是<strong>硬件</strong>级别的，不是程序员控制的</li></ul><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210102231899.png" alt="image-20221010223142729" style="zoom:25%;"></li><li><p>超标量</p><ul><li><p>为了能够利用多发射，系统必须找出能够同时执行的指令</p></li><li><p>在预测技术中，编译器或者处理器对一条指令进行猜测，然后在<strong>猜测</strong>的基础上执行代码。（可能会猜错</p></li><li><p>例1</p><img src="C:\Users\xiaon\AppData\Roaming\Typora\typora-user-images\image-20221021205239913.png" alt="image-20221021205239913" style="zoom: 25%;"><p>如果*a_p指向的是z，那么这两条指令不能同时执行</p></li><li><p>例2</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210212110457.png" alt="image-20221021211033384" style="zoom:25%;"><p>z可能为正数也可能为负数</p><p>如果系统猜错了，必须返回并重新计算w=y</p></li></ul></li></ul><h2 id="进程和线程"><a href="#进程和线程" class="headerlink" title="进程和线程"></a>进程和线程</h2><ul><li><p><strong>进程</strong>：是运行着的程序的一个实例</p><ul><li><p><strong>多任务操作系统</strong></p><p>给人一种单一处理器系统同时运行多个程序的错觉</p><p>实际上<strong>每个进程轮流运行</strong></p><p>执行了一个时间片的时间后，他会等待一段时间直到再次运行</p></li></ul></li><li><p><strong>线程</strong></p><ul><li><p>线程包含在进程中</p></li><li><p>每个线程相互独立</p><p>当某个任务阻塞时能执行其他任务</p></li><li><p>线程间的切换比进程间的切换要快</p></li></ul></li></ul><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210102203190.png" alt="image-20221010220356970" style="zoom: 33%;"><h3 id="硬件多线程"><a href="#硬件多线程" class="headerlink" title="硬件多线程"></a>硬件多线程</h3><p>硬件多线程为系统提供一种机制，使得当前执行的任务被阻塞时，系统能够继续其他有用的工作</p><p>硬件多线程分为一下三种类型：</p><p>图示？？？？？？？？？？？？？？？？？？？？？？？？？？？？？</p><ul><li><p>细粒度（程序员不可见</p><ul><li>处理器在<strong>每条指令</strong>完成后切换线程，从而跳过被阻塞的线程</li><li><strong>优点</strong>：能够避免因为阻塞而导致机器时间的浪费</li><li><strong>缺点</strong>：执行很长一段指令的线程在执行每条指令的时候都要等待</li></ul></li><li><p>粗粒度（程序员不可见</p><ul><li>只切换那些需要<strong>等待较长时间才能完成操作而被阻塞的线程</strong></li><li><strong>优点</strong>：不需要线程间的立即切换</li><li><strong>缺点</strong>：处理器还是可能在短阻塞时空闲，线程间的切换会导致延迟</li></ul></li><li><p>同步多线程（程序员可见，可以通过写程序来控制</p><ul><li>类似于多核（真正的同时</li><li>允许多个线程同时使用多个功能单元来利用超标量处理器的性能</li><li>局限性：一个核上面不会有太多的线程，常见的是2个线程</li></ul></li></ul><h2 id="并行硬件"><a href="#并行硬件" class="headerlink" title="并行硬件"></a>并行硬件</h2><h3 id="Flynn’s-分类法"><a href="#Flynn’s-分类法" class="headerlink" title="Flynn’s 分类法"></a>Flynn’s 分类法</h3><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210212141324.png" alt="image-20221021214151218" style="zoom:33%;"><h3 id="SIMD（单指令多数据流）"><a href="#SIMD（单指令多数据流）" class="headerlink" title="SIMD（单指令多数据流）"></a>SIMD（单指令多数据流）</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>通过将<strong>数据</strong>分配给多个处理器实现并行化</p><p>使用<strong>相同的指令</strong>来操纵数据子集</p><p>这种并行称为<strong>数据并行</strong></p><p> 例子</p><ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210220916715.png" alt="image-20221022091601607" style="zoom: 33%;"></li><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210220916572.png" alt="image-20221022091638479"></li></ul><h4 id="SIMD缺点："><a href="#SIMD缺点：" class="headerlink" title="SIMD缺点："></a>SIMD缺点：</h4><ul><li>所有ALU（算术处理单元）<strong>要么执行相同的指令，要么同时处于空闲状态</strong></li><li>ALU<strong>没有指令存储器</strong></li><li>在经典的SIMD系统中，ALU必须<strong>同步</strong>操作</li><li>SIMD并行性在<strong>大型数据</strong>并行问题上非常有用，处理其他并行问题时并不优秀</li></ul><h4 id="SIMD典型应用"><a href="#SIMD典型应用" class="headerlink" title="SIMD典型应用"></a>SIMD典型应用</h4><h5 id="向量处理器"><a href="#向量处理器" class="headerlink" title="向量处理器"></a>向量处理器</h5><ul><li><p>向量处理器是对<strong>数组或者数据向量</strong>进行操作，而传统的cpu是对<strong>单独数据元素或者标量</strong>进行操作</p></li><li><p>原理</p><ul><li><p>向量寄存器</p><p>能够存储由<strong>多个操作数组成的向量</strong>，并且能够同时对其内容进行操作的寄存器</p></li><li><p>向量化和流水化的功能单元</p><p>对向量中每个元素做同样的操作，这些操作需要应用到2个或以上对应元素上</p></li><li><p>向量指令</p><p>在向量上操作而不是在标量上操作</p></li><li><p>交叉存储器（不太重要</p><p>内存系统由多个内存“体”组成，每个内存体能够独立访问</p><p>如果向量中各个元素分布在<strong>不同的内存体</strong>中，那么在装入/存储连续数据时几乎能够<strong>无延迟访问</strong></p></li><li><p>步长式存储器访问和硬件的散射/聚集（不太重要</p><p>程序能够访问向量中固定间隔的元素</p></li></ul></li><li><p>向量处理器优点（理解即可</p><ul><li><p>速度快</p></li><li><p>容易使用</p></li><li><p>向量编译器擅长于识别向量化的代码</p></li><li><p>编译器也能提供代码为什么不能向量化的原因</p><p>帮助程序员重新评估代码</p></li><li><p>很高的内存带宽</p></li><li><p>每个加载的数据都会使用</p></li></ul></li><li><p>向量处理器缺点</p><ul><li>不能处理<strong>不规则</strong>的数据结构和其他并行结构<ul><li>不规则：例如加一个判断语句y&gt;0之类的就不适用</li></ul></li><li>他的<strong>可扩展性</strong>是个限制，可扩展性是指能够处理更大问题的能力<ul><li>想要提高性能就只能增加向量处理器的数量，而不是提高向量处理器的能力</li></ul></li></ul></li></ul><h5 id="GPU（图形处理单元）"><a href="#GPU（图形处理单元）" class="headerlink" title="GPU（图形处理单元）"></a>GPU（图形处理单元）</h5><ul><li>GPU使用图形处理流水线将物体表面的内部表示转化成一个像素的数组</li><li>流水线的许多阶段（通过着色函数实现）是可编程的</li><li>GPU常使用SIMD来优化性能</li><li>现在所有的GPU都使用SIMD并行<ul><li>尽管GPU不是纯粹的SIMD系统（GUP里有很多很多核</li><li>1个控制单元对应多个处理单元？？？</li></ul></li></ul><h3 id="MIMD（多指令多数据流）"><a href="#MIMD（多指令多数据流）" class="headerlink" title="MIMD（多指令多数据流）"></a>MIMD（多指令多数据流）</h3><ul><li><p>支持同时多个指令流在多个数据流上操作</p></li><li><p>通常包括完全独立的处理单元或者核，每个处理单元或者核都有自己的控制单元和ALU</p><ul><li><p>对比SIMD：</p><p>SIMD系统那些指令单元必须<strong>同步执行相同指令</strong></p></li></ul></li></ul><h4 id="MIMD分为两大类"><a href="#MIMD分为两大类" class="headerlink" title="MIMD分为两大类"></a>MIMD分为两大类</h4><h5 id="共享内存系统"><a href="#共享内存系统" class="headerlink" title="共享内存系统"></a>共享内存系统</h5><ul><li><p>一组自治的处理器通过互联网络与内存系统相互连接</p><ul><li>意思是这些处理器<strong>共享所有内存</strong>，每个处理器能够访问每个内存区域</li><li>处理器通过<strong>访问共享的数据结构</strong>来<strong>隐式的通信</strong></li></ul></li><li><p>最广泛使用的共享内存系统使用一个或者多个多核处理器</p><ul><li>在一块芯片上有多个cpu或者核</li></ul></li><li><p>图示</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210221002252.png" alt="image-20221022100256140" style="zoom: 25%;"></li><li><p>共享内存系统可以分为两类：</p><ul><li><p>UMA一致内存访问系统</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210221006036.png" alt="image-20221022100638935"></p></li><li><p>NUMA非一致内存访问系统</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210221010042.png" alt="image-20221022101055916"></p><p>这是共享内存系统，而不是分布式内存系统！！</p></li></ul></li></ul><h5 id="分布式内存系统"><a href="#分布式内存系统" class="headerlink" title="分布式内存系统"></a>分布式内存系统</h5><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210221010627.png" alt="image-20221022101027522" style="zoom: 25%;"><ul><li>集群（最广泛使用<ul><li>这些系统中的节点是通过通信网络互相连接的独立计算单元</li></ul></li></ul><h4 id="互联网络"><a href="#互联网络" class="headerlink" title="互联网络"></a>互联网络</h4><ul><li><p>在分布式内存系统和共享内存系统都扮演了一个决定性的角色</p></li><li><p>分为两类</p><p>共享内存互联网络</p><p>分布式内存互联网络</p><p>=======================草，从这往后好像都没讲草草草草</p></li></ul><h5 id="共享内存互联网络"><a href="#共享内存互联网络" class="headerlink" title="共享内存互联网络"></a>共享内存互联网络</h5><ul><li><p>总线互连</p><ul><li>需要排队，但成本低</li><li>随着连接到总线的设备数量的增加，对总线 的使用的竞争会增加，性能会下降。</li></ul></li><li><p>交换互连网络</p><p>灵活但造价高</p><p>允许<strong>不同设备</strong>之间<strong>同时</strong>进行通信</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210231827338.png" alt="image-20221023182726222" style="zoom: 50%;"><h6 id="cache一致性问题"><a href="#cache一致性问题" class="headerlink" title="cache一致性问题"></a>cache一致性问题</h6><p><img src="https://tva1.sinaimg.cn/large/008vxvgGly1h7o682zywcj30gw0osjsp.jpg" alt="image-20221031083552339"></p><p>Y1，z1为1号核的私有变量</p><p>y0为0号核的私有变量</p><p><img src="https://tva1.sinaimg.cn/large/008vxvgGly1h7o6bou6v7j30xu0e2myq.jpg" alt="image-20221031083920604"></p></li><li><p>监听cache一致性协议</p><ul><li>core0更新x时，会在总线上广播更新信息，core1可以监听到（⚠️不是直接传递x的信息，而是广播共享）</li></ul></li><li><p>基于目录的cache一致性协议</p><ul><li>使用一个叫做<strong>目录</strong>的结构，存储每个内存行的状态</li><li>当一个变量需要更新时，就会查询目录，并将所有包含该<strong>变量</strong>的高速缓存行设置为非法</li></ul></li></ul><h5 id="分布式内存互联网络"><a href="#分布式内存互联网络" class="headerlink" title="分布式内存互联网络"></a>分布式内存互联网络</h5><h6 id="分为两种：直接互连（要求全部掌握）、间接互连"><a href="#分为两种：直接互连（要求全部掌握）、间接互连" class="headerlink" title="分为两种：直接互连（要求全部掌握）、间接互连"></a>分为两种：直接互连（要求全部掌握）、间接互连</h6><ul><li><p>直接互连</p><ul><li><p>每个交换器与一个处理器-内存对直接相连，交换器之间也互相连接</p></li><li><p>一个环</p><p>可以同时进行通信</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210231835525.png" alt="image-20221023183528459" style="zoom:50%;"></li><li><p>二维环面网络</p><p>可以同时进行信息交换的节点更多</p><p>造价更高</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210231836859.png" alt="image-20221023183648796" style="zoom: 33%;"></li></ul></li><li><p>间接互连</p><ul><li><p>交换器不一定与处理器直接相连</p></li><li><p>间接网络中相对简单的例子：</p><ul><li><p>交叉开关矩阵</p><p>区分<strong>共享互联网络</strong>中的交叉开关矩阵</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210231925922.png" alt="image-20221023192547825" style="zoom:33%;"></li><li><p>Omega网络</p><p><img src="https://tva1.sinaimg.cn/large/008vxvgGly1h7o5yhbdsaj30p00nymyn.jpg" alt="image-20221031082637682"></p></li></ul></li></ul></li></ul><h6 id="一些定义（直接互连中）"><a href="#一些定义（直接互连中）" class="headerlink" title="一些定义（直接互连中）"></a>一些定义（直接互连中）</h6><ul><li><p>等分宽度（需要会计算</p><ul><li><p>衡量“同时通信的链路数目”或者“连接性”的一个标准</p></li><li><p>想象并行系统被分成两部分，每部分都 有一半的处理器或者节点。在这两部份 之间能同时发生多少通信呢？</p></li><li><p>例1：一个环的两种等分<img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210231845256.png" alt="image-20221023184507171"></p><p>等分宽度求的是最少的情况下的同时通信链路数目</p><p>这个环的等分宽度是2而不是4</p></li><li><p>例2：一个二维环面网格的等分</p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210231852261.png" alt="image-20221023185215144" style="zoom: 25%;"><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210231852924.png" alt="image-20221023185239861" style="zoom: 33%;"><p>把红线全部删掉才能把网格结构<strong>等分成两份</strong>，把两部分完全分开，由图一变成图二，要剪断<strong>8</strong>根线</p><p><strong>总结</strong>：</p><p>二维环面网格的<strong>等分宽度</strong>为<strong>2*p^(0.5)</strong></p><p>p表示处理器的数目</p></li></ul></li><li><p>延迟</p><ul><li>指从发送源开始传送数据到目的地开始接受数据之间的时间</li><li><img src="https://tva1.sinaimg.cn/large/008vxvgGly1h7o611kz3fj30qm0ikwg2.jpg" alt="image-20221031082906633"></li></ul></li><li><p>带宽</p><ul><li>传输数据的速度</li><li>通常用兆位每秒或者兆字节每秒来表示</li></ul></li><li><p>等分带宽</p><ul><li>等分带宽=等分宽度×带宽</li><li>用来衡量网络的质量</li><li>不是计算连接两个等分之间的链路数，而是计算链路的带宽</li></ul></li></ul><h6 id="几个直接互连的例子"><a href="#几个直接互连的例子" class="headerlink" title="几个直接互连的例子"></a>几个直接互连的例子</h6><ul><li><p>全相连网络</p><ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210231906924.png" alt="image-20221023190651837"></li><li>等分宽度太大，造价高，不切实际</li></ul></li><li><p>超立方体</p><ul><li>假如维度为d，则有2^d（p=2^d）个节点，等分宽度为p➗2</li></ul></li></ul><table><thead><tr><th>结构</th><th>图片</th><th>等分宽度（p为交换器数目）</th></tr></thead><tbody><tr><td>环</td><td><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210231915846.png" alt="image-20221023191549780" style="zoom:25%;"></td><td>2</td></tr><tr><td>二维环面网状结构</td><td><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210231917760.png" alt="image-20221023191734686" style="zoom:25%;"></td><td><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210231916152.png" alt="image-20221023191644099" style="zoom:25%;"></td></tr><tr><td>超立方体</td><td><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210231918391.png" alt="image-20221023191804315" style="zoom:25%;"></td><td><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/202210231918813.png" alt="image-20221023191829753" style="zoom:25%;"></td></tr></tbody></table><p>当p&gt;4时，性能：超立方体结构&gt;二维环面网状结构、环</p><h4 id=""><a href="#" class="headerlink" title=""></a></h4><hr><p>下面这块是从w4l2开始的，中间部分老师跳过了</p><h2 id="并行算法分析"><a href="#并行算法分析" class="headerlink" title="并行算法分析"></a>并行算法分析</h2><h3 id="基本指标-🌟必考"><a href="#基本指标-🌟必考" class="headerlink" title="基本指标(🌟必考)"></a>基本指标(🌟必考)</h3><ul><li><p>串行算法评价：</p><ul><li>算法时间复杂度表示为<strong>输入规模</strong>的函数</li></ul></li><li><p>并行算法评价</p><ul><li><p>除了输入规模之外，还需要考虑处理器的树木、处理器相对运算速度、通信速度</p></li><li><p>运行时间</p></li><li><p>加速比：选择串行算法<strong>最优</strong>的时间和并行算法时间做除法</p><ul><li>默认串行算法和并行算法所用的处理器相同</li></ul><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102104746771.png" alt="image-20221102104746771"></p></li></ul></li></ul><h4 id="加速比"><a href="#加速比" class="headerlink" title="加速比"></a>加速比</h4><ul><li><p>例</p><ul><li><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102105459233.png" alt="image-20221102105459233"></p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102105529255.png" alt="image-20221102105529255"></p></li><li><p>初始：每个进程保存一个数，最终由一个进程保存累加和</p></li><li><p>树形结构：logn个步骤</p><p>每个步骤进行一次加法：</p><p>和一个机器字的传输</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102112240099.png" alt="image-20221102112240099"></p><p>没懂？？？</p></li></ul></li><li><p>例。加速比的计算（‼️考试容易考）</p><ul><li><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102112435610.png" alt="image-20221102112435610"></p></li><li><p>p为核数</p></li><li><p>搜索分解导致超线性的例子</p><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102113015030.png" alt="image-20221102113015030"></p></li></ul></li></ul><h5 id="阿姆达尔定律"><a href="#阿姆达尔定律" class="headerlink" title="阿姆达尔定律"></a>阿姆达尔定律</h5><ul><li><p>定义</p><ul><li>除非一个串行程序的执行几乎全部都并行化，否则不论多少可以利用的核，通过并行化所产生的加速比都会是受限的</li><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102200500482.png" alt="image-20221102200500482"></li></ul></li><li><p>例子</p><ul><li><p>理想化：0.9的可完全并行，拥有线性加速比<img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102200035338.png" alt="image-20221102200035338"></p></li><li><p>从公式可以看出来，s和串行程序运行时间是无关的</p></li></ul></li></ul><h4 id="效率"><a href="#效率" class="headerlink" title="效率"></a>效率</h4><ul><li><p>效率：度量有效计算时间</p></li><li><p><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102200902075.png" alt="image-20221102200902075"></p></li><li><p>例1</p><ul><li>n个核对n个数求和</li><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102201204778.png" alt="image-20221102201204778"></li></ul></li><li><p>例2</p><ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102201235794.png" alt="image-20221102201235794"></li></ul></li></ul><h4 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h4><ul><li><p>我们希望，保持问题规模不变时，效率不随着线程数的增加而降低，则称程序是可扩展的</p><ul><li>强扩展的</li></ul></li><li><p>问题规模增大时，效率不随着线程数的增加而降低，则称程序是可扩展的</p><ul><li>弱扩展的</li></ul></li><li><p>例子</p><p> <img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102203837409.png" alt="image-20221102203837409"></p></li><li><p>例子：快速傅立叶变换</p><ul><li><img src="https://zxn-1313885264.cos.ap-beijing.myqcloud.com/images_typora/image-20221102204141865.png" alt="image-20221102204141865"></li></ul></li></ul><h3 id="并行程序设计的复杂性"><a href="#并行程序设计的复杂性" class="headerlink" title="并行程序设计的复杂性"></a>并行程序设计的复杂性</h3><ul><li>足够的并发度</li><li>并发粒度<ul><li>独立的计算任务的大小</li></ul></li><li>局部性<ul><li>对临近数据进行计算</li><li>尽量减少数据的计算，对离得比较近的数据进行计算</li></ul></li><li>负载均衡<ul><li>每个处理器工作量相近</li></ul></li><li>协调和同步</li></ul><h3 id="并行算法额外开销"><a href="#并行算法额外开销" class="headerlink" title="并行算法额外开销"></a>并行算法额外开销</h3><ul><li>进程间通信<ul><li>也可能是线程通信</li><li>最大开销，大部分并行算法都需要</li></ul></li><li>进程空闲<ul><li>负载不均，同步操作，不能并行化的部分</li></ul></li><li>额外计算<ul><li>最优串行算法难以并行化，将很差的串行算法并行化，并行算法计算量&gt;最优串行算法</li><li>最优串行算法并行化也会产生额外计算<ul><li>并行快速傅立叶变换</li><li>旋转因子的重复计算</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 并行程序设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并行程序设计 </tag>
            
            <tag> S5课上 </tag>
            
            <tag> 知识总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人工智能导论-深度学习实验</title>
      <link href="/2022/09/13/shen-du-shen-jing-wang-luo-shi-yan-bao-gao/"/>
      <url>/2022/09/13/shen-du-shen-jing-wang-luo-shi-yan-bao-gao/</url>
      
        <content type="html"><![CDATA[<h1 id="深度神经网络实验报告"><a href="#深度神经网络实验报告" class="headerlink" title="深度神经网络实验报告"></a>深度神经网络实验报告</h1><h2 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h2><ul><li>分别使用全连接网络，卷积神经网络，循环神经网络去预测数据（图像分类） CIFAR-10 数据集</li><li>使用几个机器学习算法（自选）作为比较。 对预测结果进行分析。<ul><li>SVM算法</li><li>KNN算法</li></ul></li></ul><h2 id="CIFAR-10数据集介绍"><a href="#CIFAR-10数据集介绍" class="headerlink" title="CIFAR-10数据集介绍"></a>CIFAR-10数据集介绍</h2><h3 id="数据集内容"><a href="#数据集内容" class="headerlink" title="数据集内容"></a>数据集内容</h3><p>​CIFAR10数据集共有60000个样本，每个样本都是一张32*32像素的RGB图像（彩色图像），每个RGB图像又必定分为3个通道（R通道、G通道、B通道）。这60000个样本被分成了50000个训练样本和10000个测试样本。<br>CIFAR10数据集是用来监督学习训练的，那么每个样本就一定都配备了一个标签值（用来区分这个样本是什么），不同类别的物体用不同的标签值，CIFAR10中有10类物体，标签值分别按照0~9来区分,他们分别是飞机（ airplane ）、汽车（ automobile ）、鸟（ bird ）、猫（ cat ）、鹿（ deer ）、狗（ dog ）、青蛙（ frog ）、马（ horse ）、船（ ship ）和卡车（ truck ）。</p><p>​CIFAR10数据集的内容，如下图所示：</p><table><thead><tr><th>物体种类</th><th></th><th></th><th></th><th></th><th></th><th>图片</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>airplane</td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane1.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane2.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane3.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane4.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane5.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane6.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane7.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane8.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane9.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane10.png" alt="img"></td></tr><tr><td>automobile</td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile1.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile2.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile3.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile4.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile5.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile6.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile7.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile8.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile9.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile10.png" alt="img"></td></tr><tr><td>bird</td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird1.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird2.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird3.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird4.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird5.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird6.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird7.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird8.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird9.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird10.png" alt="img"></td></tr><tr><td>cat</td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat1.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat2.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat3.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat4.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat5.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat6.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat7.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat8.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat9.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat10.png" alt="img"></td></tr><tr><td>deer</td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer1.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer2.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer3.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer4.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer5.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer6.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer7.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer8.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer9.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer10.png" alt="img"></td></tr><tr><td>dog</td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog1.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog2.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog3.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog4.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog5.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog6.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog7.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog8.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog9.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog10.png" alt="img"></td></tr><tr><td>frog</td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog1.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog2.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog3.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog4.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog5.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog6.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog7.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog8.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog9.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog10.png" alt="img"></td></tr><tr><td>horse</td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse1.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse2.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse3.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse4.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse5.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse6.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse7.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse8.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse9.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse10.png" alt="img"></td></tr><tr><td>ship</td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship1.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship2.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship3.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship4.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship5.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship6.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship7.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship8.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship9.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship10.png" alt="img"></td></tr><tr><td>truck</td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck1.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck2.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck3.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck4.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck5.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck6.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck7.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck8.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck9.png" alt="img"></td><td><img src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck10.png" alt="img"></td></tr></tbody></table><h3 id="数据集的结构组成"><a href="#数据集的结构组成" class="headerlink" title="数据集的结构组成"></a>数据集的结构组成</h3><p>​CIFAR10数据集结构组成可分为这四个部分：</p><ul><li>·train_x:(50000, 32, 32, 3)——训练样本</li><li>·train_y:(50000, 1)——训练样本标签</li><li>·test_x:(10000, 32, 32, 3)——测试样本</li><li>·test_y:(10000, 1)——测试样本标签</li></ul><h3 id="数据集的下载方式"><a href="#数据集的下载方式" class="headerlink" title="数据集的下载方式"></a>数据集的下载方式</h3><p>​在Keras中已经内置了多种公共数据集，其中就包含CIFAR-10数据集，如图所示。</p><table><thead><tr><th>序号</th><th>名称</th><th>说明</th></tr></thead><tbody><tr><td>1</td><td>boston_housing</td><td>波士顿房价数据集</td></tr><tr><td>2</td><td>CIFAR10</td><td>10种类别的图片集</td></tr><tr><td>3</td><td>CIFAR100</td><td>100种类别的图片集</td></tr><tr><td>4</td><td>MNIST</td><td>手写数字图片集</td></tr><tr><td>5</td><td>Fashion-MNIST</td><td>10种时尚类别的图片集</td></tr><tr><td>6</td><td>IMDB</td><td>电影点评数据集</td></tr><tr><td>7</td><td>Reuters</td><td>路透社新闻数据集</td></tr></tbody></table><p>​所以可以直接调用 tf.keras.datasets.cifar10，直接下载数据集。</p><p>​使用方法如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> keras<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> cifar10 <span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span> cifar10<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>返回：<ul><li>2元组：<ul><li>x_train，x_test：具有形状（num_samples ，3，32，32）的RGB图像数据的uint8数组。</li><li>y_train，y_test：uint8具有形状（num_samples，）的类别标签数组（范围0-9中的整数）。</li></ul></li></ul></li></ul><h2 id="深度神经网络的说明"><a href="#深度神经网络的说明" class="headerlink" title="深度神经网络的说明"></a>深度神经网络的说明</h2><h3 id="全连接网络"><a href="#全连接网络" class="headerlink" title="全连接网络"></a>全连接网络</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>​全连接神经网络(DNN)是最朴素的神经网络，它的网络参数最多，计算量最大。</p><p>​DNN的结构不固定，一般神经网络包括<strong>输入层</strong>、<strong>隐藏层</strong>和<strong>输出层</strong>，一个DNN结构只有一个输入层，一个输出层，输入层和输出层之间的都是隐藏层。每一层神经网络有若干神经元(下图中蓝色圆圈)，层与层之间神经元相互连接，层内神经元互不连接，而且下一层神经元连接上一层所有的神经元。</p><p><img src="https://img2018.cnblogs.com/blog/1433301/201907/1433301-20190705201830865-1883632909.png" alt="img"></p><p>​<strong>优点</strong>：由于DNN几乎可以拟合任何函数，所以DNN的非线性拟合能力非常强。往往深而窄的网络要更节约资源。</p><p>​<strong>缺点：</strong>DNN不太容易训练，需要大量的数据，很多技巧才能训练好一个深层网络。</p><h4 id="模型说明"><a href="#模型说明" class="headerlink" title="模型说明"></a>模型说明</h4><p>​创建模型的代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#构建模型</span>layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">for</span> nodes <span class="token keyword">in</span> nodes_per_layer<span class="token punctuation">:</span> layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>nodes<span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 使用 l2 正则化 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001)</span> layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout_rate<span class="token punctuation">)</span><span class="token punctuation">)</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>model <span class="token operator">=</span> keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>layers<span class="token punctuation">)</span><span class="token comment">#编译模型</span>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span>tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span><span class="token number">0.001</span><span class="token punctuation">)</span><span class="token punctuation">,</span>              loss<span class="token operator">=</span><span class="token string">'sparse_categorical_crossentropy'</span><span class="token punctuation">,</span>              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​我创建了一个 Keras中的Sequential 模型，包含了多个网络层：</p><ul><li><p>模型的第一层是一个 Flatten 层，它将输入的多维数组展平为一维数组，以便于后面的全连接层使用。</p></li><li><p>模型包含了四个 Dense 层。Dense 层就是全连接层，每个节点都与上一层的所有节点连接。第一个 Dense 层有 512 个节点，使用 relu 作为激活函数。第二个 Dense 层有 256 个节点，也使用 relu 作为激活函数。第三个 Dense 层有 128 个节点，也使用 relu 作为激活函数。第四个 Dense 层有 64 个节点，也使用 relu 作为激活函数。在每两层中间插入一个dropout层，这样一来，第一层的输出将对第二层实现Dropout正则化，后续层与此类似。</p></li><li><p>模型的最后一层是一个 Softmax层，有 10 个节点。Softmax函数将多个标量映射为一个概率分布，其输出的每一个值范围在(0,1)。这在分类问题中很常用，因为概率值可以表示模型的置信度。</p></li></ul><p>​配置训练方法时，训练时用的优化器、损失函数和准确率评测标准如下：</p><ul><li><strong>optimizer 优化器</strong>：选用Adam，学习率lr设置为0.001</li><li><strong>loss 损失函数</strong>：选用稀疏的分类交叉熵 sparse_categorical_crossentropy</li><li><strong>metrics</strong> <strong>评价函数</strong>：选用准确率accuracy</li></ul><h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><p>​起初我训练模型时没有数据增强，代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">overfitting_hist <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">,</span>                             use_multiprocessing<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment">#绘制训练和测试准确率</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>overfitting_hist<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>overfitting_hist<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'accuracy'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230114184455601.png" alt="image-20230114184455601" style="zoom:33%;"><p>​此时，训练精度远高于测试精度（高达35%），此外，测试精度要么是静态的，要么是下降的，这意味着模型严重过度拟合。训练速度很快，使模型达到 50% 左右，但是它之后开始过度拟合，所以我切换到对生成数据进行训练，这些数据速度较慢，但不太可能导致过度拟合。</p><p>​因此，我切换到增强数据，在 50000 个样本上进行训练，在 10000 个样本上进行验证，代码如下；</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">augmented <span class="token operator">=</span> datagen<span class="token punctuation">.</span>flow<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>hist <span class="token operator">=</span> model<span class="token punctuation">.</span>fit_generator<span class="token punctuation">(</span>augmented<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#绘制精度曲线</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>hist<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>hist<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'accuracy'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​结果如图：</p><img src="/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230114172148881.png" alt="image-20230114172148881" style="zoom: 33%;"><p>​绘制出的测试和训练集的准确率图像如下：</p><img src="/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_a4ac7ee4-f1da-47fa-b870-c0efba09a04g.jpg" alt="img_v2_a4ac7ee4-f1da-47fa-b870-c0efba09a04g" style="zoom: 33%;"><p>​从上图可以看出；一开始，测试精度优于训练精度，但在第一个时期之后，训练精度仅比测试精度高出 4%，这意味着模型不会过度拟合。</p><h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><p>​模型在测试集上的准确率如下：</p><p>![image-20230114172401122](/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230114172401122.png)</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>​一方面使用未增强的数据集进行训练会导致过度拟合；另一方面立即使用增强的数据集会导致欠拟合。因此，我想出的最好的方法是先使用未增强的数据进行训练，直到导致过度拟合，然后切换到增强数据。该模型在增强数据上进行训练的难度较小，因为它已经了解了类的许多主要特征，然后数据增强使学习变得更加困难，但不易过度拟合。</p><h4 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">from</span> tensorflow <span class="token keyword">import</span> keras<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> os<span class="token comment">#超参数</span>epochs <span class="token operator">=</span> <span class="token number">30</span>nodes_per_layer <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1024</span><span class="token punctuation">,</span><span class="token number">512</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">]</span>layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>batch_size <span class="token operator">=</span> <span class="token number">64</span>dropout_rate <span class="token operator">=</span> <span class="token number">0.3</span>num_classes <span class="token operator">=</span> <span class="token number">10</span><span class="token comment">#===========================</span><span class="token comment">#一.加载和标准化数据</span><span class="token comment">#===========================</span><span class="token comment">#加载数据</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span> keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>cifar10<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#标准化图像</span>x_train <span class="token operator">=</span> tf<span class="token punctuation">.</span>map_fn<span class="token punctuation">(</span><span class="token keyword">lambda</span> image<span class="token punctuation">:</span> tf<span class="token punctuation">.</span>image<span class="token punctuation">.</span>per_image_standardization<span class="token punctuation">(</span>image<span class="token punctuation">)</span><span class="token punctuation">,</span> x_train<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>x_train <span class="token operator">=</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>run<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>x_test <span class="token operator">=</span> tf<span class="token punctuation">.</span>map_fn<span class="token punctuation">(</span><span class="token keyword">lambda</span> image<span class="token punctuation">:</span> tf<span class="token punctuation">.</span>image<span class="token punctuation">.</span>per_image_standardization<span class="token punctuation">(</span>image<span class="token punctuation">)</span><span class="token punctuation">,</span> x_test<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>x_test <span class="token operator">=</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>run<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span><span class="token comment">#===========================</span><span class="token comment">#二.构建、编译模型</span><span class="token comment">#===========================</span><span class="token comment">#我没有使用 l2 正则化，因为它使使用生成的数据的学习速度变慢并且不会减少过度拟合</span>layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">for</span> nodes <span class="token keyword">in</span> nodes_per_layer<span class="token punctuation">:</span> layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>nodes<span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 使用 l2 正则化 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001)</span> layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout_rate<span class="token punctuation">)</span><span class="token punctuation">)</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>model <span class="token operator">=</span> keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>layers<span class="token punctuation">)</span><span class="token comment">#编译模型</span>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span>tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span><span class="token number">0.001</span><span class="token punctuation">)</span><span class="token punctuation">,</span>              loss<span class="token operator">=</span><span class="token string">'sparse_categorical_crossentropy'</span><span class="token punctuation">,</span>              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment">#===========================</span><span class="token comment">#三.训练模型</span><span class="token comment">#===========================</span><span class="token comment">#1. 没有使用数据增强</span><span class="token comment">#这种训练速度很快，使模型达到 50% 左右，但是它之后开始过度拟合，所以我切换到对生成数据进行训练，这些数据速度较慢，但不太可能导致过度拟合</span>overfitting_hist <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">,</span>                             use_multiprocessing<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment">#绘制训练和测试准确率</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>overfitting_hist<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>overfitting_hist<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'accuracy'</span><span class="token punctuation">)</span><span class="token comment">#数据生成</span><span class="token comment">#定义数据生成器以生成更多样化的数据</span>datagen <span class="token operator">=</span> keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>image<span class="token punctuation">.</span>ImageDataGenerator<span class="token punctuation">(</span>    featurewise_center<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    featurewise_std_normalization<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    samplewise_center<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    samplewise_std_normalization<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    rotation_range<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>    zoom_range<span class="token operator">=</span><span class="token number">0.07</span><span class="token punctuation">,</span>    width_shift_range<span class="token operator">=</span><span class="token number">0.15</span><span class="token punctuation">,</span>    height_shift_range<span class="token operator">=</span><span class="token number">0.15</span><span class="token punctuation">,</span>    shear_range<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>    horizontal_flip<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment">#初始化数据生成所需的变量</span>datagen<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span><span class="token comment">#2. 使用数据增强</span><span class="token comment">#使用生成的数据训练模型以避免过度拟合</span>augmented <span class="token operator">=</span> datagen<span class="token punctuation">.</span>flow<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>hist <span class="token operator">=</span> model<span class="token punctuation">.</span>fit_generator<span class="token punctuation">(</span>augmented<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#绘制精度曲线</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>hist<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>hist<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'accuracy'</span><span class="token punctuation">)</span><span class="token comment">#===========================</span><span class="token comment">#四.评估模型</span><span class="token comment">#===========================</span>test_loss<span class="token punctuation">,</span> test_acc <span class="token operator">=</span> model<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Test accuracy:'</span><span class="token punctuation">,</span> test_acc<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h4><p>如果用全连接神经网络处理大尺寸图像具有三个明显的缺点：</p><ul><li>将图像展开为向量会丢失空间信息；</li><li>参数过多效率低下，训练困难；</li><li>大量的参数也很快会导致网络过拟合。</li></ul><p>而使用卷积神经网络可以很好地解决上面的三个问题。</p><p>一个卷积神经网络主要由以下 5 层组成：</p><ul><li>数据输入层/ Input layer</li><li>卷积计算层/ CONV layer</li><li>ReLU 激励层 / ReLU layer</li><li>池化层 / Pooling layer</li><li>全连接层 / FC layer</li></ul><h4 id="模型说明-1"><a href="#模型说明-1" class="headerlink" title="模型说明"></a>模型说明</h4><p>​创建模型的代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#卷积层 32个3*3的卷积核 激活函数relu输入形状32*32*3</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#池化层 2*2</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>MaxPooling2D<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#卷积层 64个3*3的卷积核 激活函数relu</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#池化层 2*2</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>MaxPooling2D<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#卷积层 64个3*3的卷积核 激活函数relu</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#展平</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#全连接层 64个神经元 激活函数relu</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#全连接层 10个神经元 激活函数softmax</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"softmax"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​我创建了一个 Keras中的Sequential 模型，包含了多个网络层：</p><ul><li>模型的第一部分包含了三个卷积层，分别包含32个3 * 3、 64 个 3 * 3 、64 个 3 * 3的卷积核，和两个2*2的池化层</li><li>模型包含了一个Flatten层，将前一层的输出展平为一维张量。</li><li>模型的最后包含了两个全连接层，一个全连接层有64个神经元，激活函数为relu；另一个全连接层有10个单元，激活函数为softmax。</li></ul><h4 id="训练模型-1"><a href="#训练模型-1" class="headerlink" title="训练模型"></a>训练模型</h4><p>​将epochs设置为40，在 50000 个样本上进行训练，在 10000 个样本上进行验证，代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_images<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">,</span>                    validation_data<span class="token operator">=</span><span class="token punctuation">(</span>test_images<span class="token punctuation">,</span> test_labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_15fd61a7-6d0f-450f-8139-1f3aed0baa1g.jpg" alt="img_v2_15fd61a7-6d0f-450f-8139-1f3aed0baa1g" style="zoom:33%;"><img src="/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_bed4c8ec-8e37-4ffd-8839-abcb5183df1g.jpg" alt="img_v2_bed4c8ec-8e37-4ffd-8839-abcb5183df1g" style="zoom:33%;"><h4 id="模型评估-1"><a href="#模型评估-1" class="headerlink" title="模型评估"></a>模型评估</h4><p>​模型在测试集上的准确率如下：</p><p>![img_v2_f2bb69a8-ea6b-4cb8-b976-53e6907612ag](/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_f2bb69a8-ea6b-4cb8-b976-53e6907612ag.jpg)</p><h4 id="完整代码-1"><a href="#完整代码-1" class="headerlink" title="完整代码"></a>完整代码</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token comment">#===========================</span><span class="token comment">#一.加载和标准化数据</span><span class="token comment">#===========================</span><span class="token punctuation">(</span>train_images<span class="token punctuation">,</span> train_labels<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>test_images<span class="token punctuation">,</span> test_labels<span class="token punctuation">)</span> <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>cifar10<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>train_images<span class="token punctuation">,</span> test_images <span class="token operator">=</span> train_images <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> test_images <span class="token operator">/</span> <span class="token number">255.0</span><span class="token comment">#===========================</span><span class="token comment">#二.构建、编译模型</span><span class="token comment">#===========================</span>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#卷积层 32个3*3的卷积核 激活函数relu输入形状32*32*3</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#池化层 2*2</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>MaxPooling2D<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#卷积层 64个3*3的卷积核 激活函数relu</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#池化层 2*2</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>MaxPooling2D<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#卷积层 64个3*3的卷积核 激活函数relu</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#展平</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#全连接层 64个神经元 激活函数relu</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#全连接层 10个神经元 激活函数softmax</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"softmax"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">"adam"</span><span class="token punctuation">,</span>              loss<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"accuracy"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment">#===========================</span><span class="token comment">#三.训练模型</span><span class="token comment">#===========================</span>history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_images<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">,</span>                    validation_data<span class="token operator">=</span><span class="token punctuation">(</span>test_images<span class="token punctuation">,</span> test_labels<span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"accuracy"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"accuracy"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"val_accuracy"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"val_accuracy"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"Epoch"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Accuracy"</span><span class="token punctuation">)</span><span class="token comment"># plt.ylim([0.0, 1])</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#===========================</span><span class="token comment">#四.评估模型</span><span class="token comment">#===========================</span>test_loss<span class="token punctuation">,</span> test_acc <span class="token operator">=</span> model<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>test_images<span class="token punctuation">,</span> test_labels<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Test accuracy:'</span><span class="token punctuation">,</span> test_acc<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><h4 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h4><p>​RNNs的目的使用来处理序列数据。在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。</p><p>​RNNs之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。理论上，RNNs能够对任何长度的序列数据进行处理。但是在实践中，为了降低复杂性往往假设当前的状态只与前面的几个状态相关，下图便是一个典型的RNNs： </p><p><img src="https://img-blog.csdn.net/20150921225357857" alt="img"></p><h4 id="模型说明-2"><a href="#模型说明-2" class="headerlink" title="模型说明"></a>模型说明</h4><p>​创建模型的代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">"adam"</span><span class="token punctuation">,</span>              loss<span class="token operator">=</span><span class="token string">"sparse_categorical_crossentropy"</span><span class="token punctuation">,</span>              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​我创建了一个 Keras中的Sequential 模型，包含了多个网络层：</p><ul><li>模型的第一层是一个双向 LSTM 层，它具有 32 个神经元，并返回序列。</li><li>模型的第二层是一个双向 LSTM 层，它也具有 32 个神经元。</li><li>第三层是一个池化层，它可以防止过拟合。</li><li>第四层是一个 Flatten 层，它把输入展平。</li><li>第五层是一个全连接层，它具有 128 个神经元和 ReLU 激活函数。</li><li>第六层是一个池化层，防止过拟合。</li><li>第七层是一个全连接层，它具有 64 个神经元和 ReLU 激活函数。</li><li>最后一层是一个全连接层，它具有 10 个神经元，激活函数为 softmax 。</li></ul><h4 id="训练模型-2"><a href="#训练模型-2" class="headerlink" title="训练模型"></a>训练模型</h4><p>​将epochs设置为10，在 50000 个样本上进行训练，在 10000 个样本上进行验证，代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_images<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> batch<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>                    validation_data<span class="token operator">=</span><span class="token punctuation">(</span>test_images<span class="token punctuation">,</span> test_labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>![img_v2_eaf8d3c3-da3d-424e-a759-5b43f838492g](/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_eaf8d3c3-da3d-424e-a759-5b43f838492g.jpg)</p><p>​绘制出的图像如下；</p><img src="/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_80be0f27-a53b-45a9-9658-d9b2eb8f6abg.jpg" alt="img_v2_80be0f27-a53b-45a9-9658-d9b2eb8f6abg" style="zoom:33%;"><h4 id="模型评估-2"><a href="#模型评估-2" class="headerlink" title="模型评估"></a>模型评估</h4><p>​模型在测试集上的准确率如下：</p><p>![img_v2_6d331709-f95a-4fd1-8010-865f4940bbeg](/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_6d331709-f95a-4fd1-8010-865f4940bbeg.jpg)</p><h4 id="完整代码-2"><a href="#完整代码-2" class="headerlink" title="完整代码"></a>完整代码</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token comment">#===========================</span><span class="token comment">#一.加载和标准化数据</span><span class="token comment">#===========================</span><span class="token punctuation">(</span>train_images<span class="token punctuation">,</span> train_labels<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>test_images<span class="token punctuation">,</span> test_labels<span class="token punctuation">)</span> <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>cifar10<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>train_images<span class="token punctuation">,</span> test_images <span class="token operator">=</span> train_images <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> test_images <span class="token operator">/</span> <span class="token number">255.0</span><span class="token comment">#===========================</span><span class="token comment">#二.构建、编译模型</span><span class="token comment">#===========================</span>model <span class="token operator">=</span> keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">"adam"</span><span class="token punctuation">,</span>              loss<span class="token operator">=</span><span class="token string">"sparse_categorical_crossentropy"</span><span class="token punctuation">,</span>              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment">#===========================</span><span class="token comment">#三.训练模型</span><span class="token comment">#===========================</span>history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_images<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> batch<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>                    validation_data<span class="token operator">=</span><span class="token punctuation">(</span>test_images<span class="token punctuation">,</span> test_labels<span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"accuracy"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"accuracy"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"val_accuracy"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"val_accuracy"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"Epoch"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Accuracy"</span><span class="token punctuation">)</span><span class="token comment"># plt.ylim([0.0, 1])</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#===========================</span><span class="token comment">#四.评估模型</span><span class="token comment">#===========================</span>test_loss<span class="token punctuation">,</span> test_acc <span class="token operator">=</span> model<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>test_images<span class="token punctuation">,</span> test_labels<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Test accuracy:'</span><span class="token punctuation">,</span> test_acc<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="机器学习算法"><a href="#机器学习算法" class="headerlink" title="机器学习算法"></a>机器学习算法</h2><h3 id="SVM算法"><a href="#SVM算法" class="headerlink" title="SVM算法"></a>SVM算法</h3><h4 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h4><p>​支持向量机（SVM）是一种广泛使用的监督学习算法，可用于分类和回归。它的工作原理是找到一条决策边界，将不同类别的数据尽可能分开。</p><p>​使用 SVM 分类 CIFAR-10 数据集的一般步骤如下：</p><ul><li>准备数据：将 CIFAR-10 数据集加载到 Python 中，并将其转换为适合模型使用的格式。可能需要对图像进行预处理，例如缩放或归一化。</li><li>选择 SVM 模型：选择要使用的 SVM 模型（例如线性 SVM 或非线性 SVM）和决策函数。</li><li>训练模型：使用训练数据训练 SVM 模型。</li><li>评估模型：使用测试数据评估SVM模型</li></ul><h4 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h4><p>​我构建的svm模型如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">svm</span><span class="token punctuation">(</span>c<span class="token punctuation">,</span> kernel<span class="token operator">=</span><span class="token string">'poly'</span><span class="token punctuation">,</span> degree<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> kernel <span class="token operator">==</span> <span class="token string">'poly'</span><span class="token punctuation">:</span>        svm_model <span class="token operator">=</span> SVC<span class="token punctuation">(</span>kernel<span class="token operator">=</span>kernel<span class="token punctuation">,</span> degree<span class="token operator">=</span>degree<span class="token punctuation">,</span> C<span class="token operator">=</span>c<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        svm_model <span class="token operator">=</span> SVC<span class="token punctuation">(</span>kernel<span class="token operator">=</span>kernel<span class="token punctuation">,</span> C<span class="token operator">=</span>c<span class="token punctuation">)</span>    svm_model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    <span class="token keyword">return</span> svm_model<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​我使用 sklearn 中的 SVC，构建 SVM 分类模型，令核函数的默认参数为 poly 多项式核函数。</p><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>当惩罚参数C为5时，模型的训练结果如下：</p><p>![img_v2_20b0e3f9-349d-4fb8-aa81-24387d864bfg](/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_20b0e3f9-349d-4fb8-aa81-24387d864bfg.jpg)</p><p>此时模型在测试集上的正确率为0.47</p><p>手动调节惩罚参数C，模型在测试集上的正确率也在不断变化如下：</p><table><thead><tr><th>C</th><th>1</th><th>3</th><th>5</th><th>7</th><th>10</th><th>50</th><th>100</th></tr></thead><tbody><tr><td>acc</td><td>0.42</td><td>0.45</td><td>0.47</td><td>0.38</td><td>0.43</td><td>0.41</td><td>0.41</td></tr></tbody></table><p>从上表可知，当C=5时正确率最高。</p><h4 id="模型评估-3"><a href="#模型评估-3" class="headerlink" title="模型评估"></a>模型评估</h4><p>模型在训练集和测试集上的准确率如下：</p><p>![img_v2_b2e15a6d-7ab1-4d11-b9fb-a207b972160g](/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_b2e15a6d-7ab1-4d11-b9fb-a207b972160g.jpg)</p><p>模型在测试集上的表现如下：</p><p>![img_v2_20b0e3f9-349d-4fb8-aa81-24387d864bfg](/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_20b0e3f9-349d-4fb8-aa81-24387d864bfg.jpg)</p><h4 id="完整代码-3"><a href="#完整代码-3" class="headerlink" title="完整代码"></a>完整代码</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>pipeline <span class="token keyword">import</span> make_pipeline<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> StandardScaler<span class="token punctuation">,</span> MinMaxScaler<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>svm <span class="token keyword">import</span> SVC<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> GridSearchCV<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> classification_report<span class="token keyword">from</span> time <span class="token keyword">import</span> time<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">from</span> tensorflow <span class="token keyword">import</span> keras<span class="token comment">#===========================</span><span class="token comment">#加载和标准化数据</span><span class="token comment">#===========================</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>cifar10<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>X_train <span class="token operator">=</span> X_train<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10000</span><span class="token punctuation">]</span>y_train <span class="token operator">=</span> y_train<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10000</span><span class="token punctuation">]</span>X_test <span class="token operator">=</span> X_test<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2000</span><span class="token punctuation">]</span>y_test <span class="token operator">=</span> y_test<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2000</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>X_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>X_test<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>X_train_length <span class="token operator">=</span> X_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> X_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> X_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>X_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X_train_length<span class="token punctuation">)</span>X_test <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X_test<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X_train_length<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>X_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>X_test<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment">#===========================</span><span class="token comment">#构建、编译模型</span><span class="token comment">#===========================</span><span class="token keyword">def</span> <span class="token function">svm</span><span class="token punctuation">(</span>c<span class="token punctuation">,</span> kernel<span class="token operator">=</span><span class="token string">'poly'</span><span class="token punctuation">,</span> degree<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> kernel <span class="token operator">==</span> <span class="token string">'poly'</span><span class="token punctuation">:</span>        svm_model <span class="token operator">=</span> SVC<span class="token punctuation">(</span>kernel<span class="token operator">=</span>kernel<span class="token punctuation">,</span> degree<span class="token operator">=</span>degree<span class="token punctuation">,</span> C<span class="token operator">=</span>c<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        svm_model <span class="token operator">=</span> SVC<span class="token punctuation">(</span>kernel<span class="token operator">=</span>kernel<span class="token punctuation">,</span> C<span class="token operator">=</span>c<span class="token punctuation">)</span>    svm_model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    <span class="token keyword">return</span> svm_model<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    kernel <span class="token operator">=</span> <span class="token string">'rbf'</span>    c <span class="token operator">=</span> <span class="token number">5.0</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'%s(C=%.1f):'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>kernel<span class="token punctuation">,</span> c<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment">#===========================</span>    <span class="token comment">#训练模型</span>    <span class="token comment">#===========================</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'正在训练...'</span><span class="token punctuation">)</span>    start <span class="token operator">=</span> time<span class="token punctuation">(</span><span class="token punctuation">)</span>    model <span class="token operator">=</span> svm<span class="token punctuation">(</span>c<span class="token punctuation">,</span> kernel<span class="token operator">=</span>kernel<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'训练完成所需时间： %.2fs'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment">#===========================</span>    <span class="token comment">#评估模型</span>    <span class="token comment">#===========================</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Train accuracy: %.2f'</span> <span class="token operator">%</span> model<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Test accuracy: %.2f'</span> <span class="token operator">%</span> model<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 测试集</span>    y_pred <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>classification_report<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> y_pred<span class="token punctuation">,</span> digits<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="KNN算法"><a href="#KNN算法" class="headerlink" title="KNN算法"></a>KNN算法</h3><h4 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h4><p>​KNN算法的实现步骤如下：</p><ul><li><p>计算待分类的测试数据的特征向量与已知类别的训练样本的特征向量的欧氏距离。距离公式为：</p><img src="/Users/zhangxiaoni/Library/Application Support/typora-user-images/image-20230115011443638.png" alt="image-20230115011443638" style="zoom:50%;"></li><li><p>将距离从小到大排序。</p></li><li><p>去前k个值并统计k个值中每个类别出现的频数。</p></li><li><p>频数最大的训练样本的类别即为测试样本的与预测类别。</p></li></ul><h4 id="模型构建-1"><a href="#模型构建-1" class="headerlink" title="模型构建"></a>模型构建</h4><p>​调用 sklearn 中的 KNeighborsClassifier，设置 n_neighbors=10，即临近的节点数量 为 10。构建模型的代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">knn <span class="token operator">=</span> KNeighborsClassifier<span class="token punctuation">(</span>n_neighbors<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="模型训练-1"><a href="#模型训练-1" class="headerlink" title="模型训练"></a>模型训练</h4><p>模型训练过程如下：</p><p>![img_v2_aa0a16c1-2447-4fb2-8528-fbfffd3a832g](/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_aa0a16c1-2447-4fb2-8528-fbfffd3a832g.jpg)</p><h4 id="模型评估-4"><a href="#模型评估-4" class="headerlink" title="模型评估"></a>模型评估</h4><p>模型在训练集和测试集上的准确率如下：</p><img src="/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_789e6f0a-9b73-4e15-849a-abca1faabf4g.jpg" alt="img_v2_789e6f0a-9b73-4e15-849a-abca1faabf4g" style="zoom: 67%;"><h4 id="完整代码-4"><a href="#完整代码-4" class="headerlink" title="完整代码"></a>完整代码</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>neighbors <span class="token keyword">import</span> KNeighborsClassifier<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> time <span class="token keyword">import</span> time<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token comment">#===========================</span><span class="token comment">#加载和标准化数据</span><span class="token comment">#===========================</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>cifar10<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>X_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>X_test<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>X_train_length <span class="token operator">=</span> X_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> X_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> X_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>X_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X_train_length<span class="token punctuation">)</span>X_test <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X_test<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X_train_length<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>X_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>X_test<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'training...'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"neighbors_settings:"</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>start <span class="token operator">=</span> time<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#===========================</span><span class="token comment">#构建、编译模型</span><span class="token comment">#===========================</span>knn <span class="token operator">=</span> KNeighborsClassifier<span class="token punctuation">(</span>n_neighbors<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token comment">#===========================</span><span class="token comment">#训练模型</span><span class="token comment">#===========================</span>knn<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token comment">#===========================</span><span class="token comment">#评估模型</span><span class="token comment">#===========================</span>train_acc <span class="token operator">=</span> knn<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>test_acc <span class="token operator">=</span> knn<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'training finished in %.2fs'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'train_accuracy:'</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'test_acccuacy:'</span><span class="token punctuation">,</span> test_acc<span class="token punctuation">)</span>y_pred <span class="token operator">=</span> knn<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>classification_report<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> y_pred<span class="token punctuation">,</span> digits<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="结果对比分析"><a href="#结果对比分析" class="headerlink" title="结果对比分析"></a>结果对比分析</h2><h3 id="可视化图像对比"><a href="#可视化图像对比" class="headerlink" title="可视化图像对比"></a>可视化图像对比</h3><p>​不同深度神经网络的表现对比如下：</p><table><thead><tr><th>深度神经网络</th><th>acc</th></tr></thead><tbody><tr><td>全连接网络</td><td><img src="/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_a4ac7ee4-f1da-47fa-b870-c0efba09a04g.jpg" alt="img_v2_a4ac7ee4-f1da-47fa-b870-c0efba09a04g" style="zoom:33%;"></td></tr><tr><td>卷积神经网络</td><td><img src="/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_bed4c8ec-8e37-4ffd-8839-abcb5183df1g.jpg" alt="img_v2_bed4c8ec-8e37-4ffd-8839-abcb5183df1g" style="zoom:33%;"></td></tr><tr><td>循环神经网络</td><td><img src="/Users/zhangxiaoni/Library/Containers/com.bytedance.macos.feishu/Data/Library/Application Support/LarkShell/sdk_storage/407120424b82e9e84d5a9fe5984b53e7/resources/images/img_v2_80be0f27-a53b-45a9-9658-d9b2eb8f6abg.jpg" alt="img_v2_80be0f27-a53b-45a9-9658-d9b2eb8f6abg" style="zoom:33%;"></td></tr></tbody></table><p>​从以上结果可知，深度神经网络中，模型适配度：卷积神经网络&gt;全连接网络&gt;循环神经网络</p><h3 id="acc对比"><a href="#acc对比" class="headerlink" title="acc对比"></a>acc对比</h3><p>​不同算法在测试集上的准确率如下：</p><table><thead><tr><th>算法</th><th>accuracy</th></tr></thead><tbody><tr><td>全连接网络</td><td>0.6424</td></tr><tr><td>卷积神经网络</td><td>0.6864</td></tr><tr><td>循环神经网络</td><td>0.4893</td></tr><tr><td>SVM算法</td><td>0.47</td></tr><tr><td>KNN算法</td><td>0.3386</td></tr></tbody></table><p>​从以上结果可知，本实验所选取的方案中，模型适配度：卷积神经网络&gt;全连接网络&gt;循环神经网络&gt;SVM算法&gt;KNN算法</p><p>​可以直观看出，本实验中深度学习模型效果好于机器学习模型。</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能导论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> S5课上 </tag>
            
            <tag> 实验报告 </tag>
            
            <tag> 人工智能导论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>blog 说明</title>
      <link href="/2021/09/07/hello-world/"/>
      <url>/2021/09/07/hello-world/</url>
      
        <content type="html"><![CDATA[<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generatehexo g<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deployhexo d<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="hexo常用命令"><a href="#hexo常用命令" class="headerlink" title="hexo常用命令"></a>hexo常用命令</h2><pre class="line-numbers language-text" data-language="text"><code class="language-text">hexo new "name"       # 新建文章hexo new page "name"  # 新建页面hexo g                # 生成页面hexo d                # 部署hexo g -d             # 生成页面并部署hexo s                # 本地预览hexo clean            # 清除缓存和已生成的静态文件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Font-matter"><a href="#Font-matter" class="headerlink" title="Font-matter"></a>Font-matter</h2><table><thead><tr><th>配置选项</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>title</td><td><code>Markdown</code> 的文件标题</td><td>文章标题，强烈建议填写此选项</td></tr><tr><td>date</td><td>文件创建时的日期时间</td><td>发布时间，强烈建议填写此选项，且最好保证全局唯一</td></tr><tr><td>author</td><td>根 <code>_config.yml</code> 中的 <code>author</code></td><td>文章作者</td></tr><tr><td>img</td><td><code>featureImages</code> 中的某个值</td><td>文章特征图，推荐使用图床(腾讯云、七牛云、又拍云等)来做图片的路径.如: <code>http://xxx.com/xxx.jpg</code></td></tr><tr><td>top</td><td><code>true</code></td><td>推荐文章（文章是否置顶），如果 <code>top</code> 值为 <code>true</code>，则会作为首页推荐文章</td></tr><tr><td>cover</td><td><code>false</code></td><td><code>v1.0.2</code>版本新增，表示该文章是否需要加入到首页轮播封面中</td></tr><tr><td>coverImg</td><td>无</td><td><code>v1.0.2</code>版本新增，表示该文章在首页轮播封面需要显示的图片路径，如果没有，则默认使用文章的特色图片</td></tr><tr><td>password</td><td>无</td><td>文章阅读密码，如果要对文章设置阅读验证密码的话，就可以设置 <code>password</code> 的值，该值必须是用 <code>SHA256</code> 加密后的密码，防止被他人识破。前提是在主题的 <code>config.yml</code> 中激活了 <code>verifyPassword</code> 选项</td></tr><tr><td>toc</td><td><code>true</code></td><td>是否开启 TOC，可以针对某篇文章单独关闭 TOC 的功能。前提是在主题的 <code>config.yml</code> 中激活了 <code>toc</code> 选项</td></tr><tr><td>mathjax</td><td><code>false</code></td><td>是否开启数学公式支持 ，本文章是否开启 <code>mathjax</code>，且需要在主题的 <code>_config.yml</code> 文件中也需要开启才行</td></tr><tr><td>summary</td><td>无</td><td>文章摘要，自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</td></tr><tr><td>categories</td><td>无</td><td>文章分类，本主题的分类表示宏观上大的分类，只建议一篇文章一个分类</td></tr><tr><td>tags</td><td>无</td><td>文章标签，一篇文章可以多个标签</td></tr><tr><td>keywords</td><td>文章标题</td><td>文章关键字，SEO 时需要</td></tr><tr><td>reprintPolicy</td><td>cc_by</td><td>文章转载规则， 可以是 cc_by, cc_by_nd, cc_by_sa, cc_by_nc, cc_by_nc_nd, cc_by_nc_sa, cc0, noreprint 或 pay 中的一个</td></tr></tbody></table><h3 id="最简示例"><a href="#最简示例" class="headerlink" title="最简示例"></a>最简示例</h3><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">title</span><span class="token punctuation">:</span> typora<span class="token punctuation">-</span>vue<span class="token punctuation">-</span>theme主题介绍<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-07 09:25:00</span><span class="token punctuation">---</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="最全示例"><a href="#最全示例" class="headerlink" title="最全示例"></a>最全示例</h3><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">title</span><span class="token punctuation">:</span> typora<span class="token punctuation">-</span>vue<span class="token punctuation">-</span>theme主题介绍<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-07 09:25:00</span><span class="token key atrule">author</span><span class="token punctuation">:</span> 赵奇<span class="token key atrule">img</span><span class="token punctuation">:</span> /source/images/xxx.jpg<span class="token key atrule">top</span><span class="token punctuation">:</span> <span class="token boolean important">true</span><span class="token key atrule">cover</span><span class="token punctuation">:</span> <span class="token boolean important">true</span><span class="token key atrule">coverImg</span><span class="token punctuation">:</span> /images/1.jpg<span class="token key atrule">password</span><span class="token punctuation">:</span> 8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92<span class="token key atrule">toc</span><span class="token punctuation">:</span> <span class="token boolean important">false</span><span class="token key atrule">mathjax</span><span class="token punctuation">:</span> <span class="token boolean important">false</span><span class="token key atrule">summary</span><span class="token punctuation">:</span> 这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要<span class="token key atrule">categories</span><span class="token punctuation">:</span> Markdown<span class="token key atrule">tags</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> Typora  <span class="token punctuation">-</span> Markdown<span class="token punctuation">---</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="文章分类"><a href="#文章分类" class="headerlink" title="文章分类"></a>文章分类</h2><p>每篇博文都是按照 <strong>“科目/项目-名字”</strong> 的形式命名的。其中归类是按照 <strong>“科目/项目”</strong> 进行归类</p><p>对于标签，呈现 <strong>“2 + n”</strong> 的结构，<strong>2</strong> 指的是每个博文都有一个“科目/项目”标签，一个“时间标签”，每个学期都会分为“课上”，“复习”，“假期”3个标签，用于细化时间节点。<strong>n</strong> 文章的属性，应该有以下几种：</p><table><thead><tr><th align="left">类别</th><th align="left">解释</th></tr></thead><tbody><tr><td align="left">工具总结</td><td align="left">包括数学工具，办公工具，各种软件的使用方法</td></tr><tr><td align="left">知识总结</td><td align="left">是对所学内容的系统梳理，这种文章应该都是比较系统、全面，但是也比较死板</td></tr><tr><td align="left">直观理解</td><td align="left">这方面记录了很多直观化的知识，但是相应的，也比较跳脱，不严谨</td></tr><tr><td align="left">实验报告</td><td align="left">这是课程实验的一些总结报告</td></tr><tr><td align="left">吃喝玩乐</td><td align="left">生活记录</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> blog说明 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blog 说明 </tag>
            
            <tag> S6课上 </tag>
            
            <tag> 工具使用 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
